=============================================================================
* Goals of the design:

** 1) Must be asynchronous, with distinct initiation and finalization
calls.

** 2) Must be "suitable" for implementing the UPC collectives.
Specifically, we need to be able to enforce at least the minimum
input/output constraints of the UPC sync_mode.

** 3) Must be suitable for use on all (reasonable) GASNet platforms,
including those with unaligned segments, progress via polling only,
etc.

** 4) Must be suitable for use in hierarchical implementations on
CLUMPS (i.e. pthreaded clients and SysV shared memory bypass inside
GASNet when it's enabled)

** 5) Must be "efficient" in terms of synchronization - for instance
avoid use of a full barrier where point-to-point synchronization is
faster and still sufficient for correctness.  Of course, this goal
will be achieved differently on each platform and the reference
version need not be infinitely tunable.

** 6) Must admit optimizations using likely hardware features where
appropriate (barriers, broadcast, RDMA-atomics).

** 7) Must NOT break any existing GASNet semantics.

** 8) MAY assume that calls which initiate collective calls are, in
fact, collective with calls made in the same order on all nodes, in
the same barrier phase. Synchronization calls may happen in different
orders on different nodes, but collectives will never be in-flight
across a barrier notify/wait interval.

** 9) Must allow for address (and related) arguments which are not
"single-valued" in the sense used by the UPC-spec.  For instance, when
implementing the Titanium exchange operation we have an operation
which is similar to a upc_all_gather_all, except that each node knows
only its own source and destination addresses, not the addresses on
the other addresses.  It is reasonable to expect that GASNet can move
the addresses around more efficiently than a network-independent
GASNet client.  (With AMs, GASNet might even move the data w/o ever
moving the addresses.)

** 10) MAY perform "extra" communication and/or computation where
there is some net savings (examples include implementing
upc_all_scatter() with a broadcast and then discarding some of the
data at each node, or use of full barriers where hardware support
makes them superior to pairwise synchronization.)

*** 11) Must be suitable for implementing the collective support
required by Titanium (which may include future extensions to utilize
the new GASNet support) and Co-Array FORTRAN.

*** 12) Should provide support for aggregating the message and
synchronization traffic of separate in-flight collectives, both within
the GASNet implementation and with explicit help from the client. For
example, the compiler should be able to transform several source-level
IN_ALLSYNC|OUT_ALLSYNC collectives issued in a row which it determines
to be data-independent into a series of GASNet calls expressing that
all the collectives should be worked on simultaneously - ideally
causing the implementation to piggy-back any data movement together,
and use a single entry and exit barrier for the entire group of
collectives.

*** 13) Should support operating on in-segment and out-of-segment data

*** 14) Should allow extension to variable-contribution data movement
collectives in the future.

*** 15) Should allow extension to team-based collectives in the future.

=============================================================================
* Collectives and barriers

At present it is not legal for GASNet collectives and barriers to be
outstanding simultaneously.  This means that collectives may not be
initiated between gasnet_barrier_notify() and the following _try() or
_wait(), and collectives initiated before a gasnet_barrier_notify()
call must also be synced before it.

We may relax this constraint in a future specification.


=============================================================================
* Collectives and handlers

GASNet collectives may not be initiated or synchronized from within AM
handlers.  This constraint will NOT be relaxed.


=============================================================================
* Handles

All GASNet collective initiation functions will have blocking and
explicit-handle non-blocking versions.  The blocking versions exist
because in some cases hardware support (such as the blocking hardware
barrier in elan) can implement them more efficiently than the trivial
one-liner.  We omit implicit-handle operations because we anticipate
collectives will be used primarily by library authors with the number
of outstanding operations being small and known at compile time, not
by code generators where the number of operations may be potentially
large and unknown at compile time.

For many reasons we desire to have a handle type which is distinct
from the gasnet_handle_t used for Extended API.  For the moment I'm
assuming a gasnet_coll_* prefix for functions.  So we introduce
"gasnet_coll_handle_t" and "GASNET_COLL_INVALID_HANDLE" to the GASNet
specification.  Meanings are MOSTLY analogous to the existing explicit
handle, except that they are per-node, not per-thread.  We keep the
semantic that a call which is intended to initiate a non-blocking
collective can return GASNET_COLL_INVALID_HANDLE if the operation was
completed synchronously.  We also keep the rule that the invalid
handle is all zero bits.

As with gasnet_handle_t, a gasnet_coll_handle_t is "dead" once it has
been successfully synced and may not be used in future synchronization
operations.


=============================================================================
* Synchronization modes

In order to efficiently implement the UPC collectives, the GASNet
collectives will include the same nine sync modes that are described
in the UPC collectives specification (though names and binary
representations need not follow UPC).  To keep the synchronization
calls lightweight, the synchronization mode will be passed to the
initiation function, but not to the synchronization function.

Here, using the UPC naming, are the three input sync modes given in
terms of GASNet semantics:

+ IN_NOSYNC: All data movement may begin on any GASNet node as soon as
any node has entered the collective initiation function.

+ IN_MYSYNC: Each block of data movement may begin as soon as both its
source and destination nodes have entered the collective initiation
function.

+ IN_ALLSYNC: Data movement may begin only when all nodes have entered
the collective initiation function.

Here are the three output sync modes, with the corresponding GASNet
semantics:

+ OUT_NOSYNC: The sync of a collective handle can succeed at any time
  so long as the last thread to sync does not do so until all data
  movement has completed.

+ OUT_MYSYNC: The sync of a collective handle can succeed as soon as
  all data movement has completed to and from local data areas
  specified in the call (note that movement to or from buffers
  internal to the implementation and local to the node might still be
  taking place, for instance in tree-based broadcasts).

+ OUT_ALLSYNC: The sync of a collective handle can succeed as soon as
  all data movement has completed to and from all data areas specified
  in the call.  This is weaker than a full barrier, since the
  equivalent of a zero-byte broadcast is sufficient.

In the presence of partial information (see Partial Information,
below) the descriptions above "all data areas specified in the call"
means the union of the information available from all nodes.

For the output syncs, I think the following names are more appropriate
to GASNet than the UPC names (where the prefix is TBD):

+ UPC_OUT_ALLSYNC -> *_ALL    implies all buffers are safe for reuse.
+ UPC_OUT_MYSYNC  -> *_LOCAL  implies local buffers are reusable
+ UPC_OUT_NOSYNC  -> *_NONE   implies no specific buffers are reusable,
                              until the last thread enters the next
                              collective (barrier included)


=============================================================================
* Bulk vs. non-bulk data reuse

Consistent with the VIS (Vector, Indexed and Strided) interface, the
collectives are specified only for "bulk" data lifetimes.  This means
that if the client reads or writes source or destination memory areas
between the initiation and synchronization of that memory, the results
are undefined.  As noted in "Synchronization Modes" the point at which
the memory (as opposed to the collective operation) is considered
synchronized depends on the synchronization mode.

+ _OUT_ALLSYNC: memory on all nodes is synchronized when the
collective operation is synchronized on any node.

+ _OUT_MYSYNC: memory on each node is synchronized when the
collective operation is synchronized on the same node.

+ _OUT_NOSYNC: memory is synchronized when the collective operation is
synchronized on all nodes.


=============================================================================
* Bulk vs. non-bulk alignment

The GASNet collectives for data movement do not assume any data
alignment (consistent with UPC and with the GASNet-VIS interface).

The computational collectives (reduce, prefix_reduce and sort)
requires alignment correct for the operation's data type.


=============================================================================
* Synchronization calls

We'll support the same family of explicit handle try and wait calls as
for the gets and puts.  See the section "Synchronization Modes" for a
description of what success of a sync call implies.

void gasnet_coll_wait_sync(gasnet_coll_handle_t handle);
int gasnet_coll_try_sync(gasnet_coll_handle_t handle);

void gasnet_coll_wait_sync_all(gasnet_coll_handle_t *handles, size_t n);
int gasnet_coll_try_sync_all(gasnet_coll_handle_t *handles, size_t n);

void gasnet_coll_wait_sync_some(gasnet_coll_handle_t *handles, size_t n);
int gasnet_coll_try_sync_some(gasnet_coll_handle_t *handles, size_t n);

Synchronization of a GASNet collective is NOT itself a collective
operation and the returns from the synchronization calls on individual
nodes may occur as early as permitted by the sync mode argument passed
to the collective initiation function.  Specifically, synchronization
calls need not occur in the same order on all nodes. 


=============================================================================
* Metadata lifetime

Where arguments are passed by reference to the collective initiation
functions, the argument values must remain unchanged between the
initiation and synchronization of the collective operation on the node
owning the referenced memory.  If this meta data lies in the GASNet
segment, clients should take care to prevent remote modification of
the meta-data, especially since a remote node may sync before the local
node has finished with the meta-data.


=============================================================================
* Progress

We expect the non-blocking collectives to make progress in
gasnet_AMPoll() or the sync call as the worst case.  A separate
section addresses how this will interface with the conduit
implementation to make progress in gasnet_AMPoll() or asynchronously
when possible.


=============================================================================
* Collectives and threads

Collectives must be initiated in the same order on all nodes.  To
execute a collective operation the collective initiation functions
should be called exactly once (i.e. by one representative thread) on
each node.  To disambiguate the order in which collectives are
initiated by multi-threaded clients, the client must serialize its own
calls to the collective initiation functions to ensure that at most one
thread per node is ever inside a GASNet collective initiation function at a
time.

The type gasnet_coll_handle_t specifies a handle which may be
synchronized from any thread, regardless of which thread initiated the
collective.  However, the collective synchronization calls may not be
called for the *same* handle concurrently from multiple threads on the
same node.

Handles are node-specific - specifically, each node gets a separate
handle_t for the operation, which it independently synchronizes in any
order with respect to other outstanding collectives. Handles may not
be passed across nodes.

=============================================================================
* Data segment

For the present we want to implement only cases where the source and
destination of the data movement collectives are in the GASNet
segment.  However, we also wish to allow a path for extension.  So, we
specify the following flags

 + *_DST_IN_SEGMENT -> If set, this bit is an assertion by the client
   that the destination address argument(s) to this collective
   operation lie in the GASNet segment.

 + *_SRC_IN_SEGMENT -> If set, this bit is an assertion by the client
   that the source address argument(s) to this collective operation
   lie in the GASNet segment.

It is an error to set either of these flag bits when any portion of
the corresponding memory lies outside the GASNet segment.

The current specification *REQUIRES* that these bits are *BOTH* set,
and thus currently only supports collective operations on data within
the GASNet segment.  This restriction should be relaxed in some future
revision.


=============================================================================
* "Single-valued" arguments

To implement things like Titanium's Exchange operation, we want a
mechanism to deal with the case that not all nodes know all addresses.
One approach would be to require the client to move addresses around
to construct a call to the GASNet collectives which is "single-valued"
in the sense used in the specification of the UPC collectives.
However, there exist a number of implementation choices which would
allow the GASNet conduit to perform collectives with only partial
information on each node, and to do so more efficiently than having
the client perform a gather_all just to collect the single-valued
arguments.  We also want to be able to use the fact that a client
implementing the UPC collectives *will* provide us with single-valued
arguments.

The GASNet collectives are *NOT* limited to being called with
single-valued arguments.  We will have flag values indicating if
address arguments to the collective initiation function are given
in the UPC or Titanium style.

 + *_LOCAL -> The arguments provided by the local initiation call
 include correct local addresses, but not correct remote addresses.

 + *_SINGLE -> The arguments provided by the initiation calls on all
 nodes include correct local and remote addresses, and they agree.

In the future we may wish to add *_GLOBAL, which would mix freely with
other nodes specifying *_LOCAL.  This would allow some nodes to know
all addresses while others knew only local ones.  We have no consumers
for this at present, and thus don't specify it yet.
 
If any node calls with the _SINGLE flags, then all nodes must do so,
and all addresses must agree across all nodes.

The 'nbytes', 'root' (for broadcast, scatter and gather) and
synchronization mode must agree across all nodes regardless of the
_LOCAL, _SINGLE flag.


=============================================================================
* Number outstanding

There exists some concern that implementing synchronization using PUTS
rather than AMs may require bounding the number of collective ops which
are outstanding on the network and/or forcing ops to be completed (inside
the conduit) in the order issued.  It has been determined that none of
this should be visible to the client:

The interface will expose a limit of no less than 2^16-1. The implementation
may internally use back-pressure flow control at a smaller limit, but
initiation calls must always independently make progress without any
synchronization action by the client on any node.

Sync calls must be non-collective to support a number of important
paradigms (eg spawn many MYSYNC collectives and compute on
the result as trysync reports them to be complete). It's not even
clear what it would mean to require a collective try sync on a MYSYNC
or NOSYNC operation (where some nodes succeed and others fail).
However, internally completing the collectives in order is acceptable
when it's necessary. That still provides significantly more flexibility
to the client than forcing the syncs to be collective.

=============================================================================
END OF NORMATIVE STUFF (MOSTLY AGREED TO)
START OF OPEN ISSUES
=============================================================================
* OPEN ISSUE: "Single address"

At the 2/19 UPC group meeting we (Dan and Paul) talked about having a
variable number of addresses (dst of broadcast, scatter, gather_all,
exchange and permute; and src of gather, gather_all, exchange and
permute).  This allows, for instance, for CLUMPS in which we perform
data placement directly into multiple UPC or Titanium threads which
share the address space of a single GASNet node.  This issue is
currently almost closed in that the current proposed interface covers
all the known potential clients:
 +  UPC or CAF w/ aligned segments, 1 thread/node
    (or multi-threaded w/ hierarchical data movement)
 +  UPC or CAF w/ unaligned segments, 1 thread/node
    (or multi-threaded w/ hierarchical data movement)
 +  UPC or CAF multi-threaded w/ direct data placement
 +  Titanium, 1 thread/node
    (or multi-threaded w/ hierarchical data movement)
 +  Ti multi-threaded w/ direct data placement


* OPEN ISSUE: Variable contributions

What about calls where each node may contribute a different sized
source to a gather or gather_all, or different sized dest to a scatter
(with a proper constraint on the sums).  This would be a natural(?)
"v" extension to the current two-flavor proposal.


* OPEN ISSUE: Computational collectives

Other than "aligned data w/ bulk lifetime" we've not specified
anything for reduce, prefix_reduce or sort.  An issue to be aware of
is that the UPC collectives spec allows for unequal contributions from
nodes.

DOB: we really need to sketch out the basic interface for the
computational collectives before we get to deeply entrenched in the
data movement collectives and have no way to reconcile the two.



* OPEN ISSUE: Progress

In-flight collectives should be guaranteed to make progress, provided
all nodes poll occasionally. Allowing them to make progress
independent of polling activity would be nice, but is not a firm
requirement.

Non-blocking initiation calls are collective, so in some
implementations (esp with IN_ALLSYNC) they may stall waiting for all
nodes to arrive. This specifically means that it would be an error for
a GASNet client to construct a dependency cycle where node A stalls
before reaching the collective initiation, waiting for the result of
some action that will be taken by node B when it exits the collective
initiation. We need to formalize this requirement in the spec.

* OPEN ISSUE: Co-Array FORTRAN (CAF)

The CAF "spec" does not provide any library or intrinsic collective
interfaces.  However, the FORTRAN 95 array syntax is available and MIGHT
express the collective operations (minus synchronization) in a manner
that the compiler could easily translate into gasnet_coll_*() calls.

Unfortunately, the lack of a defined interface encourages the calls like
Exchange to be hand coded with staggered indexes to avoid communications
bottle-necks.  Such hand-optimized collectives are NOT good candidates
for automatic translation by the complier.  This fact is noted in an
October 2003 paper from the group at Rice, which also state that they are
working on a specification for CAF intrinsics for collective
communication.

CAF permits team syncs and team collectives.  We need to look at that
when trying to extend GASNet for teams.

* OPEN ISSUE: Computational collectives (reduce, prefix reduce and sort).

** Typed or untyped interface

I am inclined toward an untyped interface in which the client provides
the number and size of elements, but not a data type.  This means that
GASNet will not implement any of the operators (+, *, MIN, MAX, etc.)
"inline", but must invoke a client-provided function to evaluate even
the simple operators.  I consider this an acceptable overhead, since
communication should be the dominant factor.

** "Blocked" interface to client-provided function

In the case of a cyclic array layout each block contains only a single
data element and there will be a single call to the client-provided
function.  However, in the case of block and block cyclic layouts
there is a choice between a simple interface that requires N calls to
the (prefix) reduction function, and a slightly more complicated one
which would make a single call to perform the (prefix) reduction on an
entire block.  I am in favor of this blocked interface.  The choice of
an untyped interface already requires passing of pointers to data
elements, so the blocked interface need only add one argument to
indicate the size of the block.

** Function interface

There is nothing in the UPC spec that would prohibit the app-provided
function from accessing remote memory.  Therefore I think we must NOT
evaluate functions from AM handler context.  So, we will use a
function pointer, not an AM handler index, as the mechanism for
specifying an op.  Since we have chosen an untyped interface, GASNet
will always take a function pointer from the client.  We can also take
a (void *) argument to pass through to the client-provided function.
In the case of UPC_FUNC and UPC_NONCOMM_FUNC that argument might be
the application-provided function, so the client can implement the
block-level looping described above.  For the rest of the
UPC-specified operations, I envision upcr having one reduction
function per type and using the void * argument to carry the upc_op_t.

** UPC-imposed complications (data layout)

It should not be too hard to deal with cyclic, block and block-cyclic
layouts of data by implementing block-cyclic and allowing the other
two to fall out as the two natural extremes.  Using the generality
allowed by the no-suffix and "i"-suffix data-movement collectives is
enough to describe a block layout even for unaligned segments and
multiple heaps per node.  The addition of a scalar "element size" is
already assumed by the choice of a untyped interface.  The addition of
one more scalar, "block size" allows us to express the block-cyclic
layout.  A value blk_sz==1 conveys pure-cyclic.  The UPC interfaces
have blk_size==0 to convey indefinite layout and that seems
appropriate here as well, but more on that below.

If UPC required phase==0 affinity=node0 for the source of a reduction
(and dest of a prefix reduction), then indefinite layout would be the
only corner case to deal with.  However, the UPC collective spec
allows arbitrary alignment of the input and output arrays.  The first
elements of the input need not have affinity to thread 0, nor is it
required to have a phase of 0.  On top of that, the prefix reduction
does not require the input and output arrays to have the same alignment.
This requires at least some level of generality in the interface.
Rather than go to the full generality of variable contributions to deal
with this, we can add just one more scalar.

Let us assume we have an interface which allows expression of a
block-cyclic layout, including an element size, number of elements per
node (one scalar value), block size and the addresses of the
contributions on each node.  Then to deal with the non-aligned case we
can pass a block-cyclic "bounding box" plus two additional scalars:
the index of the first element to use (as input or output), and the
total number of elements to use.  The second of these need not be a
new argument, but can instead can replace the element count which
previously described the full "bounding box".  The number of elements
in the bounding box can be easily computed if it is ever needed.

So, to express the input array requires the following 5-tuple:
  (elem_sz, elem_count, blk_sz, offset, address(es))
Where the addresses have the same freedom of expression as used in the
data-movement collectives.  When specifying the output array for a
prefix reduction the elem_size and elem_count must be the same, so we
only need the other three: blk_sz, offset and the address(es).

Note that UPC specifies the prefix reduction to have the same blk_sz
but allows for a different starting offset (affinity and phase).  I
propose to give gasnet the additional generality of different block
sizes.  Once indefinite layout is accounted for (below), this will
allow prefix_reduce+gather and scatter+prefix_reduce to become single
prefix reductions.  I expect to support this reblocking in the sort as
well.

If we don't allow for reblocking, then indefinite layout is probably
dealt with best by the client.  A reduction is just performed on the
source node, plus synchronization and sending the scalar result to the
destination node.  A prefix reduction or sort is performed by moving
the data to the destination node (if it differs from the source node)
and then the computation is performed on the destination.

If we do want to allow for reblocking, then indefinite layout is
something we need to express cleanly.  As in UPC, we can use blk_sz=0
to express the fact that a given src or dst array has indefinite
layout.  However, we would seem to need an additional argument to
indicate to which node the array has affinity.  However, the offset
needed for block-cyclic arrays is not needed for its original purpose.
Therefore, the "offset" would be a node number for the indefinite case
and would remain an element count for the other cases.

** Commutativity/Associativity

In the UPC spec it is stated that all reduction ops are assumed
associative and all are assumed commutative except UPC_NONCOMM_FUNC.
In order to allow for more generality, I hope to specify GASNet as
taking flags indicating if the operation is commutative and/or
associative.  However, I expect the initial reference implementation
will either ignore these flags and always assume the worst case, or
else will only implement the ASSOCIATIVE=1 case.

Ex:

Imaging X # Y # Z, where # is some operator implemented by f(.,.).

Let (C={T,F},A={T,F}) denote commutativity and associativity.

(F,F):
	No freedom of implementation.
	We must evaluate this as f(f(X,Y),Z)
(T,F): 
        Freedom to switch order of operands, not grouping.
        Legal options are:
	    f(f(X,Y), Z)    f(f(Y,X), Z)
	    f(Z, f(X,Y))    f(Z, f(Y,X))
(F,T):
	Freedom to change grouping, but not order.
	Legal options are f(f(X,Y), Z) and f(X, f(Y,Z))
(T,T):
	Full freedom allowing 12 options:
	    f(f(X,Y),Z)	      f(X,f(Y,Z))
	    f(f(X,Z),Y)	      f(X,f(Z,Y))
	    f(f(Y,X),Z)	      f(Y,f(X,Z))
	    f(f(Y,Z),X)	      f(Y,f(Z,X))
	    f(f(Z,X),Y)	      f(Z,f(X,Y))
	    f(f(Z,Y),X)	      f(Z,f(Y,X))

UPC allows for the last two options, with non-commutativity (F,T) only
supported for user-defined functions.

For non-associative reduction operators we are stuck doing
O(n_elems/blk_sz/P) "stages", where a stage is defined as computation
step between communications.  This class of reduction is not possible
with the UPC specification.  [For a prefix reduction we need the
partial sums and so are probably forced to perform these same stages
regardless of the operator.  I still need more info how prefix
reductions are implemented.]

For associative operators which are non-commutative (i.e. matrix
multiply on non-square matrices) we can perform partial reductions
over the blocks in parallel, but cannot reduce multiple blocks per row
on a given node w/o communication.  The communication that IS required
for a reduction can be done with a tree in O(log(P)) stages which
moves n_elems/blk_sz/P elements per node in each stage.  [This is not
the only way to do it.]

For operators which are both associative and commutative we can reduce
the multiple blocks on a node to a single scalar before starting
communication.  We can perform the full reduction in O(log(P)) stages
requiring only a single scalar communicated per node per stage.

Note that for pure-blocked layout there is no difference between
commutative and non-commutative operators.  For the case of
pure-cyclic the difference is the most extreme.

** Example

typedef void
	(gasnet_coll_reduce_fn)(void *dst,
				const void *src1, const void *src2,
				size_t count, void *arg);

gasnet_coll_handle_t
gasnet_coll_reduce_nb(
	gasnet_node_t dstnode, void *dst,
	void *src, size_t src_offset,
	size_t elem_size, size_t elem_count, size_t blk_sz,
	gasnet_coll_reduce_fn *func, void *func_arg,
	int flags);	

=============================================================================
END OF OPEN ISSUES
START OF CURRENT PROPOSAL
=============================================================================

There are two flavors of interface to each collective.  The basic (no
suffix) case moves an equal amount of contiguous data to or from each
node, with the exception of the root node in the asymmetric
collectives (broadcast, scatter and gather).  This type of interface
takes a single src address and a single dst address.  This is suitable
for the case of UPC collectives with aligned segments (and a single
UPC thread per node or hierarchical movement of data for multiple
threads), and also suitable for Titanium-style collectives in which
each node may have a distinct source or destination, but lacks
knowledge of the other addresses.

The second flavor is the "i"-suffix interfaces, some what analogous to
the gasnet_{get,put}i() family of interfaces.  These take a list of
sources and destinations.  This allows for the cases not covered by
the basic interface.  If a UPC runtime has unaligned segments, then
the list could specify exactly one address per node with the freedom
for them to differ.  If there are multiple UPC or Titanium threads in
the address space of a single GASNet node, then the lists could
provide multiple addresses per node to avoid the need for hierarchical
movement of data.  Note that this interface is designed for a static
number of images, such as one per language-level thread, and not for
varying the size of the contribution from each node dynamically.

Common to all these interfaces is the presence of either
GASNET_COLL_LOCAL or GASNET_COLL_SINGLE in the flags argument .  In
the SINGLE case we have UPC-style arguments in which the addresses are
known (and thus passed in) on all nodes.  In the LOCAL case we have
Titanium-style arguments in which only the local addresses are known,
and passed in.

Note that when SINGLE is given, then all arguments are "single valued"
providing identical information on all nodes (for arrays the contents
of the arrays must be identical, not their addresses).  When LOCAL is
given the address arguments may differ among the nodes while all
remaining arguments are "single valued".  For instance the nodes must
all agree on the "root" of a broadcast, scatter or gather, even though
only the root node will provided its address.  Of course all nodes
must agree on the nbytes and the flags.  Operation of any collective
in which the arguments fail to agree as required will be undefined.

For the "i" interfaces, there is additional metadata required to
indicate how many images exist on each node.  Since the "i" interfaces
are designed for this metadata to be static, it is provided exactly
once, through the gasnet_coll_init() function.  This function must be
call by clients before they may invoke any other gasnet_coll_*
function (even if they never use the "i" interfaces".  It is therefore
a collective operation, called by all nodes in the same order (always
first) among collectives.  It is also subject to the constraint that
the arguments agree across nodes.  As with other collectives the _init
call is made by one representative thread per node.  This is a
blocking call.  This function may be called at most once on each node
and may only be called between gasnet_attach() and gasnet_exit().

The 'images' argument to gasnet_coll_init() is an array of size_t's,
having exactly gasnet_nodes() elements.  The ith element of this array
indicates the number of images present on node i.  Images are
analogous to language-level threads in all the clients we envision,
but this need not be the case.  However, the images are always given
in "node-major" order, meaning that the blocks of data in a gather
operation (for instance) include all the images from node 0, followed
by all the images from node 1, etc..  All elements of the 'images'
array must be non-zero.  For use in the descriptions that follow,
define NImages := SUM(i=0..gasnet_nodes(), images[i]).  NImages must
be less than or equal to SSIZE_MAX (as defined by POSIX).

There is a 'flag' value of GASNET_COLL_AGGREGATE which can be used to
request that consecutive non-blocking collective initiation calls be
aggregated into a single collective operation with a single handle.
It is illegal to pass GASNET_COLL_AGGREGATE to a blocking collective
initiation call.

To simplify the definitions, we'll consider a single collective to be
a trivial one-element aggregate.  A collective initiation call is the
First member of an aggregate if either the previous collective
initiation call did not have GASNET_COLL_AGGREGATE in the 'flags', or
if it is the first collective initiation call.  A collective
initiation call is the Last member of an aggregate if it does not have
GASNET_COLL_AGGREGATE in the 'flags'.

With these definitions, a sequence of initiation calls all without the
GASNET_COLL_AGGREGATE flag are each trivially the First and Last
members of their own aggregates and each has a distinct handle.  A
sequence of initiation calls with the GASNET_COLL_AGGREGATE flag set
form a multiple-member aggregate, together with the first following
call without GASNET_COLL_AGGREGATE.

Only the Last member of an aggregate returns a valid handle.
The handle returned by any collective initiation call with
GASNET_COLL_AGGREGATE in the flags is undefined.  If a client attempts
to sync on a handle returned by any collective initiation call with
GASNET_COLL_AGGREGATE in the flags then the behavior is undefined.

Gasnet non-blocking collectives have bulk data lifetime semantics
regardless of the use of aggregation.

It is illegal to make blocking collective initiation calls between the
First and Last collective initiation calls of any aggregate.  It is
permitted to make other GASNet calls between the First and Last calls
of an aggregate, excepting those calls that would be prohibited even
in the absence of aggregation, such as the gasnet_barrier_* calls.

[OPEN/TBD: The description above (most notably the data lifetime
comment) allows for an implementation which does nothing but aggregate
the handles in the spirit of an nbi access region.  Such an
implementation could start the communications just as it would in the
absence of aggregation.  This seems the most natural semantic to me.
However, one alternative would be to say that communication MUST NOT
start until the Last initiation call.  If this is taken as far as
saying that the bulk data lifetime did not begin until the Last
member, then we allow the client to continue setting up the source
data until the Last call.]

With all of that said, here are the 11 prototypes needed to cover the
relocalization collectives, excluding permute:

void
gasnet_coll_init(const size_t images[], int init_flags);
/*
 *  images:	Array of gasnet_nodes() elements giving the number of
 *		images present on each node.
 *  init_flags:	Presently unused.  Must be 0.
 */


/* Argument conventions:
 * In the 10 prototypes that follow the argument names are used
 * consistently, so the following definitions apply to all arguments
 * with the given name:
 *
 * flags: Flags
 * 	The value of this argument must be equal across all nodes or
 *	the behavior is undefined.
 *	This argument supplies flags which determine how the operation
 *	will be performed.  This includes information about the
 *	synchronization required and the scope of addresses.
 *	Input synchronization flags are:
 *	+ GASNET_COLL_IN_NOSYNC: All data movement may begin on any
 *	  GASNet node as soon as any node has entered the collective
 *	  initiation function.
 *	+ GASNET_COLL_IN_MYSYNC: Data movement in or out of the memory
 *	  of a given node may begin no earlier than the execution of
 *	  the collective initiation function on that node.
 *	+ GASNET_COLL_IN_ALLSYNC: Data movement may begin only when
 *        all nodes have entered the collective initiation function.
 *	Output synchronization flags are:
 *	+ GASNET_COLL_OUT_NOSYNC: The sync of a collective handle may
 *	  succeed at any time so long as the last thread to sync does
 *	  not do so until all data movement has completed.
 *	+ GASNET_COLL_OUT_MYSYNC: The sync of a collective handle may
 *	  succeed as soon as all data movement has completed to and from
 *	  local data areas specified in the call (note that movement to
 *	  or from buffers internal to the implementation and local to the
 *	  node might still be taking place, for instance in tree-based
 *	  broadcasts).
 *	+ GASNET_COLL_OUT_ALLSYNC: The sync of a collective handle may
 *	  succeed as soon as all data movement has completed to and from
 *	  all data areas specified in the call.  This is weaker than a
 *	  full barrier.  For instance, the equivalent of a zero-byte
 *	  broadcast is sufficient synchronization in the case of a rooted
 *	  operation (broadcast, scatter and gather).
 *	Addressing scope flags are:
 *	+ GASNET_COLL_SINGLE: This indicates UPC-type addressing
 *	  in which every node provides addresses for the source(s) and
 *	  destination(s) on all nodes, and these arguments agree.
 *	+ GASNET_COLL_LOCAL: This indicates Titanium-type addressing
 *	  in which each node provides only the local address(es).
 *      Defaults:
 *	There are no default values for these three categories of flags.
 *	The client must provide exactly one flag value from each of the
 *	three categories above.
 *	Aggregation:
 *	If present, the flag GASNET_COLL_AGGREGATE indicates that a
 *	sequence of non-blocking collective initiation calls should be
 *	aggregated together into a operation with a single handle.
 *
 * nbytes: Byte count
 *	This is the number of bytes to be transfered per source block.
 * 	The value of this argument must be equal across all nodes or
 *	the behavior is undefined.
 *
 * {dst,src}node: Destination/Source node.
 *      This argument specifies the root node of an asymmetric collective
 *	operation: Broadcast, Scatter or Gather.
 * 	The value of this argument must be equal across all nodes or
 *	the behavior is undefined.
 *
 * {dst,src}: Destination (Source) address.
 *	For Broadcast and Scatter the source address is only applicable
 *	to the root node.  For the Gather operation the destination
 *	address is only applicable to the root node.  For the remaining
 *	operations both destination and source apply on all nodes.
 *
 * 	In the SINGLE case this argument is passed an identical value
 *	on all nodes.  This value is used as the destination (source)
 *	address on all nodes where the argument is applicable.  Where a
 *	given argument is not applicable to the local node the identical
 *	value must still be provided.
 *
 *	In the LOCAL case this argument may be passed independent values
 *	on each node.  Each node's value is used as it's destination
 *	(source) address where applicable.  Where a given argument is
 *	not applicable to the local node the argument is ignored.
 *
 * {dst,src}list: Destination (Source) address list.
 * 	In the SINGLE case this argument is an array of NImages addresses.
 *	The contents of this array must be identical across all nodes.
 *	The ith address is used as the destination (source) address of the
 *	ith image.  The correspondence between images and GASNet nodes is
 *	the one given by the gasnet_coll_init() call.
 *
 *	In the LOCAL case this argument may be passed independent values
 *	on each node.  On node i, this argument is an array of images[i]
 *	local addresses, where images[] is the argument passed to
 *	gasnet_coll_init().
 */

gasnet_coll_handle_t
gasnet_coll_broadcast_nb(void *dst,
                         gasnet_node_t srcnode, void *src,
                         size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_scatter_nb(void *dst,
                       gasnet_node_t srcnode, void *src,
                       size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_gather_nb(gasnet_node_t dstnode, void *dst,
                      void *src,
                      size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_gather_all_nb(void *dst, void *src,
                          size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_exchange_nb(void *dst, void *src,
                        size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_broadcasti_nb(void * const dstlist[],
                          gasnet_node_t srcnode, void * src,
                          size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_scatteri_nb(void * const dstlist[],
                        gasnet_node_t srcnode, void * src,
                        size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_gatheri_nb(gasnet_node_t dstnode, void * dst,
                       void * const srclist[],
                       size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_gather_alli_nb(void * const dstlist[], void * const srclist[],
                           size_t nbytes, int flags);

gasnet_coll_handle_t
gasnet_coll_exchangei_nb(void * const dstlist[], void * const srclist[],
                         size_t nbytes, int flags);

=============================================================================
END OF CURRENT PROPOSAL
START OF IMPLEMENTATION NOTES
=============================================================================
* Thoughts on progress of implementation:

While not part of the GASNet spec for users, it is important that we
have a good design for how the reference implementation will interact
with the individual conduits.  In particular I see at least three ways
in which a conduit would wish to make progress on collectives:

1) Simple polling, for instance from gasnet_AMPoll().  This would be
   what we include in template-conduit as well as pure-polling
   implementations such as mpi-conduit.
2) Blocked thread.  A conduit may wish to create an extra thread just
   for the purpose of making progress on collectives.  For this reason
   it would be nice to have something like a condition variable on
   which such a thread could block until there was something to do.
3) Interrupts.  A conduit such as LAPI may be able to switch to a
   different operating mode in which the application is interrupted
   when network traffic arrives.  This would make it more responsive
   to the network for the purpose of advancing a collective operation
   but the added overhead is not desirable for normal operation.  So,
   one might wish to enter interrupt mode when one or more
   collectives are "live" and return to polled mode when none are
   live.

There is similarity between #2 and #3 that argues for a general
approach that they can each plug into.  In fact, #1 can be implemented
over the same generalization as a part of the template-conduit.

The general solution is to have hooks (provided by the conduit) which
are called at each entry to a collective initiation function and when
the collective is completed (probably in the "kick" function called by
gasnete_poll()).

/* Routine to make progress on collectives (if any): */
extern void gasnete_poll(void);

/* Conduit-specific hooks for counting live collectives: */
extern void gasnete_collective_entry_hook(void);
extern void gasnete_collective_leave_hook(void);


Here are versions of all three designs (not guaranteed to compile). I
would put these all into the template-conduit:

1) Simple polling:


Implement the following in gasnet_extended_fwd.h:

extern gasneti_atomic_t gasnetc_collective_count;

GASNET_INLINE_MODIFIER(gasnete_collective_entry_hook)
void gasnete_collective_entry_hook(void)
{
  gasneti_atomic_increment(&gasnetc_collective_count);
}

GASNET_INLINE_MODIFIER(gasnete_collective_leave_hook)
void gasnete_collective_leave_hook(void)
{
  gasneti_atomic_decrement(&gasnetc_collective_count);
}

#define GASNETE_POLL_IF_NEEDED() \
  if_pf (gasneti_atomic_read(&gasnetc_collective_count) != 0) \
    { gasnete_poll(); }

In the core, one would then call GASNETE_POLL_IF_NEEDED() in
gasnetc_poll(), at least in template-conduit, and perhaps at other
places where it could be helpful (such as right after an AM handler is
run).


2) Blocking thread:

To do this we just need to add a little to the case above.  Note that
we are about to add condition variables to gasnet.  They should be a
natural wrapper around pthread condition variables, taking a hsl in
place of a pthread mutex.  Note also that a handler may NEVER wait on a
condition variable, only signal it.  Prototypes for gasneti_cond*
appear in the next section.

Implement the following in gasnet_extended_fwd.h:

extern gasneti_atomic_t gasnetc_collective_count;
extern gasnet_hsl_t gasnetc_collective_hsl;
extern gasneti_cond_t gasnetc_collective_cond;

GASNET_INLINE_MODIFIER(gasnete_collective_entry_hook)
void gasnete_collective_entry_hook(void)
{
  gasneti_atomic_increment(&gasnetc_collective_count);
  gasneti_assert(gasneti_atomic_read(&gasnetc_collective_count) >= 1);
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasneti_cond_signal(&gasnetc_collective_cond);
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}

GASNET_INLINE_MODIFIER(gasnete_collective_leave_hook)
void gasnete_collective_leave_hook(void)
{
  gasneti_atomic_decrement(&gasnetc_collective_count);
}

In the core, the thread would use something such as:

void gasnetc_collective_poll_thread(void)
{
  while (!EXITING()) {
    /* Note the "outer" test to reduce lock traffic.
     * In a race this can only cause extra calls to gasnete_poll().
     * Such extra calls are safe, but missing a wakeup is not.
     */
    if (gasneti_atomic_read(&gasnetc_collective_count) == 0) {
      while (!EXITING() && gasneti_atomic_read(&gasnetc_collective_count) == 0) {
         gasnet_hsl_lock(&gasnetc_collective_hsl);
         gasneti_cond_wait(&gasnetc_collective_cond, &gasnetc_collective_hsl);
         gasnet_hsl_unlock(&gasnetc_collective_hsl);
      }
    }

    if_pt (!EXITING()) {
      gasnete_poll();
    }

    // TODO: need a throttling mechanism here so that this thread
    // doesn't starve the working threads (which it may be waiting on) 
    // for CPU and lock resources
  }

  gasnetc_exit(0);
}


3) Interrupt mode

Implement the following in gasnet_extended_fwd.h:

extern unsigned gasnetc_collective_count;
extern gasnet_hsl_t gasnetc_collective_hsl;

GASNET_INLINE_MODIFIER(gasnete_collective_entry_hook)
void gasnete_collective_entry_hook(void)
{
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasnetc_collective_count++;
  if (gasnetc_collective_count == 1) {
    ENTER_INTERRUPT_MODE();
  }
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}

GASNET_INLINE_MODIFIER(gasnete_collective_leave_hook)
void gasnete_collective_leave_hook(void)
{
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasnetc_collective_count--;
  if (gasnetc_collective_count == 0) {
    LEAVE_INTERRUPT_MODE();
  }
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}




It might be tempting to merge the common ideas in #2 and #3 in to an
"edge triggered" pair of hooks, one called on the 0->1 transition and
the other on the 1->0 transition.  However, this would not work very
cleanly with the condition variable unless the hsl used to trigger the
hooks was exposed for use in the waiting thread.  I dislike that as an
unclear interface, and the code duplicated between #2 and #3 is too
small to justify such a design.


=============================================================================
* Proposal for gasneti_cond_t:

In order to permit a conduit to block waiting for something that might
happen in another thread, particularly in a handler executed in
another thread, we want something like pthread_cond_t, but which takes
an hsl in place of a pthread_mutex_t.  Note that these are for use
INTERNALLY, and we don't want to add them to the GASNet spec where
client code might use them.  (At least *I* don't want to do so)

Here is an attempt (not run through the compiler) to implement
pthread_cond_t in terms of our "reference" hsl, which is just a struct
holding a gasneti_mutex_t:

typedef
struct _gasneti_cond_t {
  pthread_cond_t cond;
} gasneti_cond_t;

#define GASNETI_COND_INITIALIZER { PTHREAD_COND_INITIALIZER }

#define gasneti_cond_init(P) pthread_cond_init(&((P)->cond))
#define gasneti_cond_destroy(P) pthread_cond_destroy(&((P)->cond))
#define gasneti_cond_signal(P) pthread_cond_signal(&((P)->cond))

#define gasneti_cond_wait(P,H) gasneti_cond_wait_mutex(P, &((H)->lock))

extern void gasneti_cond_wait_mutex(gasneti_cond_t *cond,
                                    gasneti_mutex_t *lock);



void
gasneti_cond_wait_mutex(gasneti_cond_t *cond, gasneti_mutex_t *lock)
{
  #if !GASNETI_USE_TRUE_MUTEXES
    gasneti_fatalerror("There's only one thread: waiting on condition variable => deadlock");
  #endif

  #if GASNET_DEBUG
    gasneti_assert(lock->owner == GASNETI_THREADIDQUERY());
    lock->owner = (uintptr_t)GASNETI_MUTEX_NOOWNER;

    pthread_cond_wait(&(cond->cond), &(lock->lock));

    gasneti_assert(lock->owner == (uintptr_t)GASNETI_MUTEX_NOOWNER);
    lock->owner = GASNETI_THREADIDQUERY();
  #else
    pthread_cond_wait(&(cond->cond), lock);
  #endif
}



=============================================================================
* Notes on algorithms 

Broadcast:
==========
  In [Karp93] we learn how to construct an optimal schedule for broadcast.  The
tree can be built knowing only the ratio A=ELL/max(O_s,g).  Using a heap data
structure one can determine the tree structure for a P-node broadcast in
O(log(P!)) time.  Note that O(P) < O(log(P!)) < O(P*log(P)), so the cost to
construct the schedule grows superlinearly.  However, it doesn't require any
communiction (deterministic algorithm allows all nodes to compute the same
schedule independently).
  For the case of A>>1 the communication is latency dominant and the optimal
broadcast is the naive one in which the root sends data to each other node in
sequence.  For the case of A near 1, the optimal tree shape is a Binomial
tree [Kielmann99].  Here are, for P=8, model results for the "EEL" of a
broadcast with four different tree shapes on three different networks.  Note
that these are in us and are directly comparable between networks.  However,
they are just model results, not real timings.  The "Flat" tree is the
one-level tree of the naive broadcast.  The percentages are relative to the
optimal.

		gm		vapi		mpi-conduit
Optimal		21		24		334
Binomial	21 		30 (+28%)	570 (+171%)
Binary		26 (+24%)	30 (+28%)	570 (+171%)
Flat		43 (+105%)	31 (+32%)	334

The binary tree and the "flat" tree are the easiest two to implement.  With
just these two choices, we can get within 30% of the optimal on the two fast
networks, and the flat tree is the optimal for the slow network.  I don't
have any bound on just how bad we could do with these two, or any heuristic
for choosing between them.  However, I expect to implement them as the first
two versions.

Later I hope to implement the construction of the optimal trees.  I can see
doing it two ways:
1) At init time for some fixed A, ignoring the data size.
2) At runtime, using some size-dependent (and conduit-specific) model for A
   and caching the results so it can be reused when the same size is repeated.
   Note that the data to cache would just be P*sizeof(gasnet_node_t) bytes.


Scatter:
========

I am planning to examine the use of the optimal broadcast tree to do a
tree-based scatter.  However, the fact that timings will NOT remain fixed as
the payload is changed (sending more data near the root than at the leaves)
means that the simple method for building such trees is only approximately
applicable.


Gather:
=======

The Gather is exactly the same as scatter with all the data moving in the
opposite direction.  Therefore whatever I decide for Scatter will also be used
to implement Gather.


Exchange:
=========

The final data placement is equivalent to doing P scatters, one with each
node as the root.  In a simple LogP model where we can only send a single data
item per communication operation, the optimal [Karp93] is to perform P naive
scatters in lock-step, but chosing the order of each node's sends so each node
receives exactly once per step.  The simplest such ordering is for node p to
send to ((p+1) mod P), ((p+2) mod P),... ((p+P-1) mod P).


Gather_All:
===========

The final data placement is equivalent to doing P broadcasts.  In the LogP
model with single-item sends the optimal [Karp93] is the same as described
above for Exchange, but with the same data item instead of distinct ones.

If we allow a node to send more than a single data item per send, then we can
improve on the methods above.  If we divide the communication into ceil(lg(P))
rounds (where lg denoted base-2 log) then each round can forward an
exponentially increasing amount of data (1 item in round 0, 2 items in round
1, 2^i items in round i).  At the end of the final round each node has
received 2^i items.  With the correct pairings for these sends, we can ensure
that all P <= 2^i items are received at each node (and with a little care we
can even avoid sending the "surplus" 2^i-P that exists when P is not a power
of two).  The ordering for this is that in round i (starting from 0) node p
will send to node ((p+2^i) mod P).  As it happens, this is exactly the same
communication pattern used in the Dissemination barrier [Mellor-Crummey91;
sec 3.3], but now we are also moving real data.

For small P the Exchange-like algorithm is faster, even if we assume the
timings are constant as the payload size grows in each round.  As the ratio A
(=ELL/max(O_s,g)) increases the crossover point moves toward larger P.  The
dominant term for the Exchange-like algorithm is O(P*max(O_s,g)), due to
the gap between the P sends from each node.  The dominant term for the
Dissemination-like algorithm is O(lg(P)*ELL), due to the need to wait for
data to arrive (before resending it) at each of the lg(P) rounds.
Both algorithms move the exact same number of data bytes in and out
of each node.  The Dissemination-like algorithms tries to reduce the number of
sends needed to actually move the data, but must pay a cost proportional to
ELL to wait for the needed data to arrive.


Permute:
========

Other than implementing the UPC synchronization, there is no obvious mechanism
by which GASNet can do any better than the use of Puts.


Reduce:
=======

As noted in [Karp93], the optimal communication pattern for reduction in the
LogP model is the same tree as for Broadcast.  The order and direction of the
comms are reversed and the data elements moving are partial "sums" from each
subtree.  There is no big concern over unequal contributions here, since each
node must contribute exactly 1 or 0 "sums" to the total reduction.  The
associativity requirement on operators makes things easier for us.

The UPC_NONCOMM_FUNC case might take a little extra work, since it would
restrict the trees we could legally use.  I've not tried to figure out what
the actual restriction would look like.


PrefixReduce:
=============

The UPC collectives spec allows one to take a slice of a UPC array such that
the number of elements can differ among the nodes.  The basic method I have
in mind would have each node compute the sums over each block (trivial for
blocksize==1) and use a Gather_All to transfer these sums.  To deal with the
possiblly unequal contribution we could either pad the block used in the
Gather_All, or we could implement Gather_All for varible-sized contributions
(which is already under consideration).

I have yet to start searching for literature specific to this problem, so
there may be methods much better than the Gather_All desribed above.


Sort:
=====

I've no bright ideas here.  The most naive method would seem to be Gather,
Sort on one node, Scatter.  However, the Gather and Scatter want equal
contribution from each node, and Sort does not require it.  This might be
another case for implementing collectives with variable-sized contributions.

I have yet to start searching for literature specific to this problem, so
there may be methods much better than the Gather+Scatter desribed above.


=============================================================================

Biblography:

@inproceedings{Karp93,
 author = {Richard M. Karp and Abhijit Sahay and Eunice E. Santos and Klaus
Erik Schauser},
 title = {Optimal broadcast and summation in the LogP model},
 booktitle = {Proceedings of the fifth annual ACM symposium on Parallel
algorithms and architectures},
 year = {1993},
 isbn = {0-89791-599-2},
 pages = {142--153},
 location = {Velen, Germany},
 doi = {http://doi.acm.org/10.1145/165231.165250},
 publisher = {ACM Press},
}

@inproceedings{Kielmann99,
 author = {Thilo Kielmann and Rutger F. H. Hofman and Henri E. Bal and Aske
Plaat and Raoul A. F. Bhoedjang},
 title = {MagPIe: MPI's collective communication operations for clustered wide
area systems},
 booktitle = {Proceedings of the seventh ACM SIGPLAN symposium on Principles
and practice of parallel programming},
 year = {1999},
 isbn = {1-58113-100-3},
 pages = {131--140},
 location = {Atlanta, Georgia, United States},
 doi = {http://doi.acm.org/10.1145/301104.301116},
 publisher = {ACM Press},
 }

@article{Mellor-Crummey91,
 author = {John M. Mellor-Crummey and Michael L. Scott},
 title = {Algorithms for scalable synchronization on shared-memory
multiprocessors},
 journal = {ACM Trans. Comput. Syst.},
 volume = {9},
 number = {1},
 year = {1991},
 issn = {0734-2071},
 pages = {21--65},
 doi = {http://doi.acm.org/10.1145/103727.103729},
 publisher = {ACM Press},
 }

@article{Santos02,
 author = {Eunice E. Santos},
 title = {Optimal and efficient algorithms for summing and prefix summing on
parallel machines},
 journal = {J. Parallel Distrib. Comput.},
 volume = {62},
 number = {4},
 year = {2002},
 issn = {0743-7315},
 pages = {517--543},
 doi = {http://dx.doi.org/10.1006/jpdc.2000.1698},
 publisher = {Academic Press, Inc.},
}

Verriet "Scheduling tree-structured programs in the LogP model"
