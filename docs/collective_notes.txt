=============================================================================
* Goals of the design:

** 1) Must be asynchronous, with distinct initiation and finalization
calls.

** 2) Must be "suitable" for implementing the UPC collectives.
Specifically, we need to be able to enforce at least the minimum
input/output constraints of the UPC sync_mode.

** 3) Must be suitable for use on all (reasonable) GASNet platforms,
including those with unaligned segments, progress via polling only,
etc.

** 4) Must be suitable for use in hierarchical implementations on
CLUMPS (both pthreaded and SysV)

** 5) Must be "efficient" in terms of synchronization - for instance
avoid use of a full barrier where point-to-point synchronization is
faster and still sufficient for correctness.  Of course, this goal
will be achieved differently on each platform and the reference
version need not be infinitely tunable.

** 6) Must admit optimizations using likely hardware features where
appropriate (barriers, broadcast, RDMA-atomics).

** 7) Must NOT break any existing GASNet semantics.

** 8) MAY assume that calls which initiate collective calls are, in
fact, collective with calls made in the same order on all nodes.

** 9) Must allow for address (and related) arguments which are not
"single-valued" in the sense used by the UPC-spec.  For instance, when
implementing the Titanium exchange operation we have an operation
which is similar to a upc_all_gather_all, except that each node knows
only its own source and destination addresses, not the addresses on
the other addresses.  It is reasonable to expect that GASNet can move
the addresses around more efficiently than a network-independent
GASNet client.  (With AMs, GASNet might even move the data w/o ever
moving the addresses.)

** 10) MAY perform "extra" communication and/or computation where
there is some net savings (examples include implementing
upc_all_scatter() with a broadcast and then discarding some of the
data at each node, or use of full barriers where hardware support
makes them superior to pairwise synchronization.)


=============================================================================
* Collectives and barriers

At present it is not legal for GASNet collectives and barriers to be
outstanding simultaneously.  This means that collectives may not be
initiated between gasnet_barrier_notify() and the following _try() or
_wait(), and collectives initiated before a gasnet_barrier_notify()
call must also be synced before it.

We may relax this constraint in a future specification.


=============================================================================
* Collectives and handlers

GASNet collectives may not be initiated or synchronized from within AM
handlers.  This constraint will NOT be relaxed.


=============================================================================
* Handles

All GASNet collective initiation functions will have blocking and
explicit-handle non-blocking versions.  The blocking versions exist
because in some cases hardware support (such as the blocking hardware
barrier in elan) can implement them more efficiently than the trivial
one-liner.  We omit implicit-handle operations because we anticipate
collectives will be used primarily by library authors with the number
of outstanding operations being small and known at compile time, not
by code generators where the number of operations may be potentially
large and unknown at compile time.

For many reasons we desire to have a handle type which is distinct
from the gasnet_handle_t used for Extended API.  For the moment I'm
assuming a gasnet_all_* prefix for functions.  So we introduce
"gasnet_all_handle_t" and "GASNET_ALL_INVALID_HANDLE" to the GASNet
specification.  Meanings are MOSTLY analogous to the existing explicit
handle, except that they are per-node, not per-thread.  We keep the
semantic that a call which is intended to initiate a non-blocking
collective can return GASNET_ALL_INVALID_HANDLE if the operation was
completed synchronously.  We also keep the rule that the invalid
handle is all zero bits.

As with gasnet_handle_t, a gasnet_all_handle_t is "dead" once it has
been successfully synced and may not be used in future synchronization
operations.


=============================================================================
* Synchronization modes

In order to efficiently implement the UPC collectives, the GASNet
collectives will include the same nine sync modes that are described
in the UPC collectives specification (though names and binary
representations need not follow UPC).  To keep the synchronization
calls lightweight, the synchronization mode will be passed to the
initiation function, but not to the synchronization function.

Here, using the UPC naming, are the three input sync modes given in
terms of GASNet semantics:

+ IN_NOSYNC: All data movement may begin on any GASNet node as soon as
any node has entered the collective initiation function.

+ IN_MYSYNC: Each block of data movement may begin as soon as both its
source and destination nodes have entered the collective initiation
function.

+ IN_ALLSYNC: Data movement may begin only when all nodes have entered
the collective initiation function.

Here are the three output sync modes, with the corresponding GASNet
semantics:

+ OUT_NOSYNC: The sync of a collective handle can succeed at any time
  so long as the last thread to sync does not do so until all data
  movement has completed.

+ OUT_MYSYNC: The sync of a collective handle can succeed as soon as
  all data movement has completed to and from local data areas
  specified in the call (note that movement to or from buffers
  internal to the implementation and local to the node might still be
  taking place, for instance in tree-based broadcasts).

+ OUT_ALLSYNC: The sync of a collective handle can succeed as soon as
  all data movement has completed to and from all data areas specified
  in the call.  This is weaker than a full barrier, since the
  equivalent of a zero-byte broadcast is sufficient.

In the presence of partial information (see Partial Information,
below) the descriptions above "all data areas specified in the call"
means the union of the information available from all nodes.

For the output syncs, I think the following names are more appropriate
to GASNet than the UPC names (where the prefix is TBD):

+ UPC_OUT_ALLSYNC -> *_ALL    implies all buffers are safe for reuse.
+ UPC_OUT_MYSYNC  -> *_LOCAL  implies local buffers are reusable
+ UPC_OUT_NOSYNC  -> *_NONE   implies no specific buffers are reusable,
                              until the last thread enters the next
                              collective (barrier included)


=============================================================================
* Bulk vs. non-bulk data reuse

Consistent with the VIS (Vector, Indexed and Strided) interface, the
collectives are specified only for "bulk" data lifetimes.  This means
that if the client reads or writes source or destination memory areas
between the initiation and synchronization of that memory, the results
are undefined.  As noted in "Synchronization Modes" the point at which
the memory (as opposed to the collective operation) is considered
synchronized depends on the synchronization mode.

+ _OUT_ALLSYNC: memory on all nodes is synchronized when the
collective operation is synchronized on any node.

+ _OUT_MYSYNC: memory on each node is synchronized when the
collective operation is synchronized on the same node.

+ _OUT_NOSYNC: memory is synchronized when the collective operation is
synchronized on all nodes.


=============================================================================
* Bulk vs. non-bulk alignment

The GASNet collectives for data movement do not assume any data
alignment (consistent with UPC and with the GASNet-VIS interface).

The computational collectives (reduce, prefix_reduce and sort)
requires alignment correct for the operation's data type.


=============================================================================
* Synchronization calls

We'll support the same family of explicit handle try and wait calls as
for the gets and puts.  See the section "Synchronization Modes" for a
description of what success of a sync call implies.

void gasnet_all_wait_sync(gasnet_all_handle_t handle);
int gasnet_all_try_sync(gasnet_all_handle_t handle);

void gasnet_all_wait_sync_all(gasnet_all_handle_t *handles, size_t n);
int gasnet_all_try_sync_all(gasnet_all_handle_t *handles, size_t n);

void gasnet_all_wait_sync_some(gasnet_all_handle_t *handles, size_t n);
int gasnet_all_try_sync_some(gasnet_all_handle_t *handles, size_t n);

Synchronization of a GASNet collective is NOT itself a collective
operation and the returns from the synchronization calls on individual
nodes may occur as early as permitted by the sync mode argument passed
to the collective initiation function.  (See OPEN ISSUES for one
reason we might wish to change this).


=============================================================================
* Metadata lifetime

Where arguments are passed by reference to the collective initiation
functions, the argument values must remain unchanged between the
initiation and synchronization of the collective operation on the node
owning the referenced memory.  If this meta data lies in the GASNet
segment, clients should take care to prevent remote modification of
the meta-data, especially since a remote node may sync before the local
node has finished with the meta-data.


=============================================================================
* Progress

We expect the non-blocking collectives to make progress in
gasnet_AMPoll() or the sync call as the worst case.  A separate
section addresses how this will interface with the conduit
implementation to make progress in gasnet_AMPoll() or asynchronously
when possible.


=============================================================================
* Collectives and threads

Collectives must be initiated in the same order on all nodes.  To
execute a collective operation the collective initiation functions
should be called exactly once (i.e. by one representative thread) on
each node.  To disambiguate the order in which collectives are
initiated by multi-threaded clients, the client must serialize its own
calls to the collective initiation functions to ensure that only one
thread is ever inside a GASNet collective initiation function at a
time.

The type gasnet_all_handle_t specifies a handle which maybe
synchronized from any thread, regardless of which thread initiated the
collective.  However, the collective synchronization calls may not be
called for the *same* handle simultaneously from multiple threads.


=============================================================================
* Data segment

For the present we want to implement only cases where the source and
destination of the data movement collectives are in the GASNet
segment.  However, we also wish to allow a path for extension.  So, we
specify the following flags

 + *_DST_IN_SEGMENT -> If set, this bit is an assertion by the client
   that the destination address argument(s) to this collective
   operation lie in the GASNet segment.

 + *_SRC_IN_SEGMENT -> If set, this bit is an assertion by the client
   that the source address argument(s) to this collective operation
   lie in the GASNet segment.

It is an error to set either of these flag bits when any portion of
the corresponding memory lies outside the GASNet segment.

The current specification *REQUIRES* that these bits are *BOTH* set,
and thus currently only supports collective operations on data within
the GASNet segment.  This restriction should be relaxed in some future
revision.


=============================================================================
* "Single-valued" arguments

To implement things like Titanium's Exchange operation, we want a
mechanism to deal with the case that not all nodes know all addresses.
One approach would be to require the client to move addresses around
to construct a call to the GASNet collectives which is "single-valued"
in the sense used in the specification of the UPC collectives.
However, there exist a number of implementation choices which would
allow the GASNet conduit to perform collectives with only partial
information on each node, and to do so more efficiently than having
the client perform a gather_all just to collect the single-valued
arguments.  We also want to be able to use the fact that a client
implementing the UPC collectives *will* provide us with single-valued
arguments.

The GASNet collectives are *NOT* limited to being called with
single-valued arguments.  We will have flag values indicating if
address arguments to the collective initiation function are given
in the UPC or Titanium style.

 + *_LOCAL -> The arguments provided by the local initiation call
 include correct local addresses, but not correct remote addresses.

 + *_SINGLE -> The arguments provided by the initiation calls on all
 nodes include correct local and remote addresses, and they agree.

In the future we may wish to add *_GLOBAL, which would mix freely with
other nodes specifying *_LOCAL.  This would allow some nodes to know
all addresses while others knew only local ones.  We have no consumers
for this at present, and thus don't specify it yet.
 
If any node calls with the _SINGLE flags, then all nodes must do so,
and all addresses must agree across all nodes.

The 'nbytes', 'root' (for broadcast, scatter and gather) and
synchronization mode must agree across all nodes regardless of the
_LOCAL, _SINGLE flag.


=============================================================================
END OF NORMATIVE STUFF (MOSTLY AGREED TO)
START OF OPEN ISSUES
=============================================================================
* OPEN ISSUE: "Single address"

At the 2/19 UPC group meeting we (Dan and Paul) talked about having a
variable number of addresses (dst of broadcast, scatter, gather_all,
exchange and permute; and src of gather, gather_all, exchange and
permute).  This allows, for instance, for CLUMPS in which we perform
data placement directly into multiple UPC or Titanium threads which
share the address space of a single GASNet node.  This issue is
currently almost closed in that the current proposed interface covers
all the known potential clients (unless CAF has something interesting
to offer us):
 +  UPC w/ aligned segments, 1 thread/node
    (or multi-threaded w/ hierarchical data movement)
 +  UPC w/ unaligned segments, 1 thread/node
    (or multi-threaded w/ hierarchical data movement)
 +  UPC multi-threaded w/ direct data placement
 +  Titanium, 1 thread/node
    (or multi-threaded w/ hierarchical data movement)
 +  Ti multi-threaded w/ direct data placement


* OPEN ISSUE: Variable contributions

What about calls where each node may contribute a different sized
source to a gather or gather_all, or different sized dest to a scatter
(with a proper constraint on the sums).  This would be a natural(?)
"v" extension to the current two-flavor proposal.


* OPEN ISSUE: Computational collectives

Other than "aligned data w/ bulk lifetime" we've not specified
anything for reduce, prefix_reduce or sort.  An issue to be aware of
is that the UPC collectives spec allows for unequal contributions from
nodes.


* OPEN ISSUE: Number outstanding vs. one-sided synchronization

In the case of gasnet_handle_t, we specify something along the lines
of "the implementation must support at least 2^16-1 unsynced handles".
In the case of the collectives, I expect that the memory use
associated with an any unsynced gasnet_all_handle_t may be
significant, possibly scaling as O(N) just for synchronization and
arrays of gasnet_handle_t (nbi regions don't work if you have to
spread the initiation calls over time as the collective gets polled).

On VAPI the latency to start handler execution is much higher than
polling a work of memory, even when spin-polling for AMs to arrive,
and AM requires flow control which is not needed for RDMA.  Therefore
I'd like to allow truely one-sided (non-AM) implementations of the
collectives, including the required pairwise synchronization when
enforcing IN_MYSYNC and/or OUT_MYSYNC.  To do this, a node must be
able to compute the address of some synchronization word on the remote
node without communication (beyond some initialization-time exchange).
Thus one-sided synchronization will require static allocation of
memory carved out of the GASNet segment (padding the user-requested
size at attach time).  This is a strong argument for ensuring the
required allocation is kept as small as reasonably possible.

At this point I am not optimistic about being able to do something
like the barrier implementation in which exactly 2 phases are
sufficient to handle an unbounded number of barriers.  If three
broadcasts are performed consecutively with the same root and
IN_MYSYNC and OUT_NOSYNC, then I can see the root receiving three "I
am ready to start" messages from each node (to enforce IN_MYSYNC)
before it has even processed the first one.  This makes me believe
that to bound the memory needed perform one-sided syncronization will
require limiting the number of collectives which are in-flight.

One approach would be to impose a "reasonable" limit, to be enforced
by the client.  This would be similar in spirit to
gasnet_AMMaxMedium(), being available to the client at runtime.  We'd
also document the minimum value that any conduit is permitted to
impose.  I am thinking a number around 8 is a reasonable lower bound,
knowing most clients will probably not reach it.

Unfortunately, enforcement by a client is likely to be tricky because
to deal with the one-sided synchronization problem, the limit must be
on the number of collectives outstanding GLOBALLY, not just locally.
Enforcement would likely come from adding barriers or IN_ALLSYNC flags
every 8th collective (assuming 8 is the limit).  I am not sure that
OUT_ALLSYNC is always sufficient since it only ensures data movement
has completed and IN_NOSYNC+OUT_ALLSYNC could complete on the root of
a broadcast (for example) even before the other nodes have entered the
collective.  Because of this difficulty of use, I am against imposing
such a "reasonable" global limit on the client (local limits of much
less than 2^16-1 are still possible in my mind).

The preceeding paragraph leads me to think that bounding the number of
in-flight collectives could best be done with the aide of
implementation-specific knowedge.  Pushing the enforcement into the
implementation would allow the client to see an apparent limit like
2^16-1.  Collectives that would overrun remote synchronization
resources could be queued locally, using dynamically allocated space
to store the arguments, until synchronization resources became
available.  The "flow control" could be piggybacked on the
collectives, for instance by promoting every 8th (for example)
collective to have OUT_ALLSYNC.

Implementing this "flow control" for synchronization resources is an
area that requires more thought.  I think that enforcing the kind of
in-flight limit described above MIGHT require making the sync of
collectives into a collective operation, meaning that not only
initiation calls for collective operations but also their sync calls
must execute in the same order on all nodes.  This would be needed so
that local knowledge would extrapolate to global knowledge without
requiring additional communication.  I had avoided making the syncs
collective because I was not sure what it would mean for clients,
especially multi-threaded ones which would now need to perform some
additional serialization.  I am pretty sure that we'd want to drop the
_all and _some sync calls.

One additional wrinkle is the possibility that RDMA based
synchronization might be possible without any of the flow control
issues I've described above.  Since collectives are executed in the
same order on all nodes and with identical sync flags, it is trivial
to unambiguously assign a sequence number to each pairwise INSYNC or
OUTSYNC event.  A static allocation of only N memory locations on each
node would be sufficient then to hold a counter for pairwise
synchonization with each peer (or 2*N if IN and OUT events get
separate counters).  The use of this counter would be slightly
complicated by the fact that the OUT events might not happen in the
order that they are numbered (the IN events must by the collective
nature of the initiation calls).  For instance:
   all_reduce(..., IN_MYSYNC | OUT_NOSYNC);
   all_broadcast(..., IN_NOSYNC | OUT_MYSYNC);
Because of the OUT_NOSYNC on the reduction and IN_NOSYNC on the
broadcast, it is legal (and desirable) for the broadcast to complete
on some nodes while the reduction is still being computed on another
node (or more than one).  Even counting only MYSYNC events has this
problem if the client issues non-blocking calls like:
   h1 = all_reduce_nb(..., IN_MYSYNC | OUT_MYSYNC);
   h2 = all_broadcast_nb(..., IN_NOSYNC | OUT_MYSYNC);
Again, the broadcast could complete ahead of the reduction, generating
some internal synchronization event for enforcement of the OUT_MYSYNC.

To the best of my knowledge the only way to eliminate the ambiguity
described above while keeping one-sided synchronization would be to
either have the implementation internally complete collectives only in
the order they were initiated or make the client's sync calls
collective.

There are still a lot of IFs in the text above.  It is still possible
that flow control of a limited pool of synchronization words can be
done transparently (perhaps resorting to AMs only occasionally - kind
of like firehose) without the need to make the sync calls collective.

So, I am soliciting feedback on this issue.  Are there any great ideas
on how to do one-sided synchronization?  Are there thoughts on what it
would mean to make the sync calls collective?

=============================================================================
END OF OPEN ISSUES
START OF CURRENT PROPOSAL
=============================================================================

* Two-flavors interface proposal

Dan has objected to the 5-flavored proposal as having an interface
which is much too wide.  This is especially a concern if we next add
the variable-contribution variants as well.  He has suggested that by
allowing a given function to include arguments which are unused for
various flag values, the use of LOCAL and SINGLE flags can reduce the
interface width.

If have followed this suggestion and also tried to find similarity to
the VIS interface.  In doing so, I collapse the "global" and "local"
into one interface, and "globalV" and "localV" into a second.  The
"globalN" interface is dropped because it is just a special case of
the "globalV" in which much of the information was implicit.  The
metadata savings of "globalN" over "globalV" was just the vector or
nodes.  However, assuming the thread->node layout is static (a pretty
safe assumption), this vector will be built once at upcr
initialization time and used for all calls to collectives.  I've also
realized that providing distinct src and dst node lists in the globalV
interface was a mistake, since there is no meaningful way for these
two to be different.

In naming my interfaces, I noted the similarity of the "V" interface
to the "i" (indexed) of the "VIS" interfaces.  Therefore, I am using
an "i" suffix.  If we choose to implement variable-sized contributions
then the "affinity" is to the "v" of VIS.  (If we desire to have a
special case for uniform thread layout with equidistant heaps then we
could add an "s" variant.)

So, here is the 2-flavor proposal:

There are two flavors of interface to each collective.  The basic (no
suffix) case moves an equal amount of contiguous data to or from each
node, with the exception of the root node in the asymmetric
collectives (broadcast, scatter and gather).  This type of interface
takes a single src address and a single dst address.  This is suitable
for the case of UPC collectives with aligned segments (and a single
UPC thread per node or hierarchical movement of data for multiple
threads), and also suitable for Titanium-style collectives in which
each node may have a distinct source or destination, but lacks
knowledge of the other addresses.

The second flavor is the "i"-suffix interfaces, some what analogous to
the gasnet_{get,put}i() family of interfaces.  These take a list of
sources and destinations.  This allows for the cases not covered by
the basic interface.  If a UPC runtime has unaligned segments, then
the list could specify exactly one address per node with the freedom
for them to differ.  If there are multiple UPC or Titanium threads in
the address space of a single GASNet node, then the lists could
provide multiple addresses per node to avoid the need for hierarchical
movement of data.

Common to all these interfaces is the presence of either "_LOCAL" or
"_SINGLE" in the flags argument (prefix TBD).  In the SINGLE case we
have UPC-style arguments in which the addresses are known (and thus
passed in) on all nodes.  In the LOCAL case we have Titanium-style
arguments in which only the local addresses are known, and passed in.

For the "i" interfaces (other than broadcast) the LOCAL case also
requires a rank argument which indicates where (in the total ordering
of blocks) the blocks from the local node lie.  This is a scalar
giving the rank of the first block on the local node, thus requiring
that threads are assigned to nodes with all threads on a given node
having consecutive rank.  The "i" interfaces also have a
"total_blocks" argument to indicate how many blocks exist after the
contributions from all nodes are summed.

Note that when SINGLE is given, then all arguments are "single valued"
providing identical information on all nodes (for arrays the contents
of the arrays must be identical, not their addresses).  When LOCAL is
given the address arguments and the rank (where applicable) may differ
among the nodes while all remaining arguments are "single valued".
For instance the nodes must all agree on the "root" of a broadcast,
scatter or gather, even though only the root node will provided its
address.  Of course all nodes must agree on the nbytes and the flags.
Operation of any collective in which the arguments fail to agree as
required will be undefined.

In the LOCAL case, the addresses may legally be different or the same,
but the ranks must differ such that [0..nblocks-1] is partitioned
among the nodes.  As we will see, the partitioning is more explicit in
the SINGLE case.

It *is* legal for one or more nodes in an "i" collective to provide
zero blocks to the collective.  This provides a rudimentary form of
variable-contribution interface in the same way that gasnet_puti() can
be used to emulate gasnet_putv(), given an nbytes which is the
greatest common divisor of the blocks.  Note that a node providing
zero blocks must still enter the collective with arguments that match
as required by the LOCAL or SINGLE mode.  In the LOCAL case the rank
(where applicable) of nodes providing zero blocks is ignored and the
partitioning of [0..nblocks-1] only considers those nodes providing
data blocks.

In designing the "i" interfaces there was a choice to be made between
passing multiple vector arguments, or a single vector of structs.  The
realization was that while the addresses are likely to be different
for each collective operation, the list of nodes is very likely to be
a static list reflecting the layout of UPC or Titanium threads on the
GASNet nodes.  Therefore a likely client will construct this list
exactly once at initialization.  By having multiple vector arguments
instead of a vector of structs, this constant node vector can be
reused for the entire life of the client without need to copy it or
reconstruct it multiple times.

The "listlen" argument in the "i" collectives is the number of
elements in all of the "nodelist", "srclist" and "dstlist" arguments
as applicable.  Note that every "i" collective has exactly one
"nodelist" and one "listlen", even the gather_all and exchange
operations.  This is because we are assuming the input and output
contributions are equal.  If a more general case is required, then the
"v" variants can be specified and implemented.

When an "i" collective is LOCAL, GASNet will ignore the "nodelist"
argument, since all addresses provided are implicitly local to the
calling node.  When SINGLE, GASNet will ignore the "rank" and
"nblocks" arguments since the equivalent information is present in the
"nodelist" and "listlen".

With all of that said, here are the 10 prototypes needed to cover the
relocalization collectives, excluding permute:

gasnet_all_handle_t
gasnet_all_broadcast_nb(void *dst,
                        gasnet_node_t srcnode, void *src,
                        size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_scatter_nb(void *dst,
                      gasnet_node_t srcnode, void *src,
                      size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_gather_nb(gasnet_node_t dstnode, void *dst,
                     void *src,
                     size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_gather_all_nb(void *dst, void *src,
                         size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_exchange_nb(void *dst, void *src,
                       size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_broadcasti_nb(gasnet_node_t * const nodelist[],
                                                    void * const dstlist[],
                         gasnet_node_t srcnode, void * src,
                         size_t listlen, size_t nbytes,
                         size_t rank, size_t nblocks,
                         int flags);

gasnet_all_handle_t
gasnet_all_scatteri_nb(gasnet_node_t * const nodelist[], void * const dstlist[],
                       gasnet_node_t srcnode, void * src,
                       size_t listlen, size_t nbytes,
                       size_t rank, size_t nblocks,
                       int flags);

gasnet_all_handle_t
gasnet_all_gatheri_nb(gasnet_node_t dstnode, void * dst,
                      gasnet_node_t * const nodelist[], void * const srclist[],
                      size_t listlen, size_t nbytes,
                      size_t rank, size_t nblocks,
                      int flags);

gasnet_all_handle_t
gasnet_all_gather_alli_nb(gasnet_node_t * const nodelist[],
                          void * const dstlist[], void * const srclist[],
                          size_t listlen, size_t nbytes,
                          size_t rank, size_t nblocks,
                          int flags);

gasnet_all_handle_t
gasnet_all_exchangei_nb(gasnet_node_t * const nodelist[],
                        void * const dstlist[], void * const srclist[],
                        size_t listlen, size_t nbytes,
                        size_t rank, size_t nblocks,
                        int flags);

=============================================================================
* Generalized collectives proposal

The UPC all_permute is that the source data and the permutation
indexes have the same affinity.  Therefore a simple put-based
mechanism is sufficient.  The only way in which GASNet can improve on
this is to provide for more efficient synchronization (especially for
MYSYNC) than could be obtained otherwise.

So, I am suggesting "all_put" and "all_get" be added to GASNet.
Unlike the other data movement collectives, these are always "LOCAL"
and it would be an error to pass the "SINGLE" flag.  Each node
provides almost the arguments it would to a gasnet_put_nb,
gasnet_get_nb, gasnet_puti_nb or gasnet_geti_nb.  The main difference
is the addition of the flags argument and the multi-node nature of the
{put,get}i.

All the rules given previously for collectives would apply to these as
well (such as agreeing on nbytes and flags, and the legality of having
listlen==0).

Implementing the SYNC flags for these interfaces would be potentially
simpler/more efficient if you knew how many nodes are targeting you,
even if you don't know which ones (even better if you did know).  For
instance, OUT_MYSYNC on a put requires you to know that all writes
into your local memory have completed.  Other than a full barrier, how
do you do this (even with signaling puts) when you don't know *how
many* writes are targeting your memory?  This could be done with the
one-to-one constraint, making these interfaces less useful for
implementing other collectives.  Alternatively an additional argument
could give the count of the partners.  The count is likely to be
either 0 (ex: root of a put-based broadcast or scatter), 1 (ex:
permute or non-root in a put-based broadcast or scatter), or N (ex:
put-based gather_all or exchange).  This idea sounds to me to be much
better than the one-to-one constraint.

Enforcing IN_MYSYNC requires you to know your partner(s) is(are) ready
as well (unless doing buffering of eager puts).  If they don't know
who you are then you need a round trip to ask if they are ready (or a
barrier to know that everyone is ready).  However, if the interfaces
above provided not only a count of partners who will target your
memory, but also their node numbers, then this synchronization can be
performed with one-way primitives instead.  If we have only the case
of 0, 1, or N then we need only a single 'partner_id': 0 needs none, 1
needs only one, and N needs none because the list is implicitly all
nodes.  If we anticipate using these calls for tree-structured data
movement then we'll need the fully general case of 0...N partners and
no assumption that the number of outgoing and ingoing match (in a tree
you have only one parent but a variable number of children).

Note that for static communication patterns construction of the list
of partners should always be possible without communication.  These
interfaces do not require that the communication patterns used be
static; dynamic patterns are possible as long as the dynamic decisions
made on all nodes will agree.  It is not required that such agreement
be reached without additional communication, but it certainly seems
desirable to avoid collective communication to select a collective
communication algorithm.

Here are generalized collective interfaces, assuming the most general
case:

gasnet_all_handle_t
gasnet_all_put_nb(gasnet_node_t dstnode, void *dst,
                  void *src,
		  gasnet_node_t * const partnerlist[], size_t partnercount, 
                  size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_get_nb(void *dst,
                  gasnet_node_t srcnode, void *src,
		  gasnet_node_t * const partnerlist[], size_t partnercount, 
                  size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_puti_nb(gasnet_node_t * const nodelist[], void * const dstlist[],
                   void * const srclist[],
                   size_t listlen,
		   gasnet_node_t * const partnerlist[], size_t partnercount, 
                   size_t nbytes, int flags);

gasnet_all_handle_t
gasnet_all_geti_nb(void * const dstlist[],
                   gasnet_node_t * const nodelist[], void * const srclist[],
                   size_t listlen,
		   gasnet_node_t * const partnerlist[], size_t partnercount, 
                   size_t nbytes, int flags);
                   

I like these generalized collective calls because they encode all the
"structure" of a collective communication.  Getting the "big picture"
takes the union over the arguments on all nodes.  However, with the
partner args present each node has all the information that affects it
directly.  With the use of non-blocking calls and the correct sync
flags to reflect data dependences between (for instance) stages of a
tree-based communication, these now look like excellent building
blocks for prototyping the other collectives.

On hardware w/o specialized support (broadcast, multicast, etc.)
tuning for a given network might just consist of choosing what pattern
of communication to implement using these generalized calls, as if it
could really be that easy :-)...  Use of these interfaces would
prohibit certain algorithms which adapt based only on local timing
information.  In such cases a given node cannot know in advance from
which node it will receive a given datum.  In some cases it cannot
even be certain how many times it will receive it.

An potentially interesting example of an algorithm that adapts to
timing in this way is a tree-based barrier (or reduction) in which the
last sibling to arrive is the one to notify the parent.  This can be
better than always designating the lowest rank child (for instance)
because the last arrival is known to be "live" at the time of its
arrival, while other children may be computing digits of pi for all we
know.  The parent cannot know in advance (at least not until the
next-to-last arrival) the identity of the last arrival - the node from
which it will receive notification.  To implement such a tree one
would probably use normal point-to-point communications (puts or AMs)
to notify the parent.  (I am not arguing that this is the best possible
barrier or reduction algorithm, just presenting an example that shows
that these generalized interfaces alone don't implement all
interesting collective algorithms).
=============================================================================
END OF CURRENT PROPOSAL
START OF IMPLEMENTATION NOTES
=============================================================================
* Thoughts on progress of implementation:

While not part of the GASNet spec for users, it is important that we
have a good design for how the reference implementation will interact
with the individual conduits.  In particular I see at least three ways
in which a conduit would wish to make progress on collectives:

1) Simple polling, for instance from gasnet_AMPoll().  This would be
   what we include in template-conduit as well as pure-polling
   implementations such as mpi-conduit.
2) Blocked thread.  A conduit may wish to create an extra thread just
   for the purpose of making progress on collectives.  For this reason
   it would be nice to have something like a condition variable on
   which such a thread could block until there was something to do.
3) Interrupts.  A conduit such as LAPI may be able to switch to a
   different operating mode in which the application is interrupted
   when network traffic arrives.  This would make it more responsive
   to the network for the purpose of advancing a collective operation
   but the added overhead is not desirable for normal operation.  So,
   one might wish to enter interrupt mode when one or more
   collectives are "live" and return to polled mode when none are
   live.

There is similarity between #2 and #3 that argues for a general
approach that they can each plug into.  In fact, #1 can be implemented
over the same generalization as a part of the template-conduit.

The general solution is to have hooks (provided by the conduit) which
are called at each entry to a collective initiation function and when
the collective is completed (probably in the "kick" function called by
gasnete_poll()).

/* Routine to make progress on collectives (if any): */
extern void gasnete_poll(void);

/* Conduit-specific hooks for counting live collectives: */
extern void gasnete_collective_entry_hook(void);
extern void gasnete_collective_leave_hook(void);


Here are versions of all three designs (not guaranteed to compile). I
would put these all into the template-conduit:

1) Simple polling:


Implement the following in gasnet_extended_fwd.h:

extern gasneti_atomic_t gasnetc_collective_count;

GASNET_INLINE_MODIFIER(gasnete_collective_entry_hook)
void gasnete_collective_entry_hook(void)
{
  gasneti_atomic_increment(&gasnetc_collective_count);
}

GASNET_INLINE_MODIFIER(gasnete_collective_leave_hook)
void gasnete_collective_leave_hook(void)
{
  gasneti_atomic_decrement(&gasnetc_collective_count);
}

#define GASNETE_POLL_IF_NEEDED() \
  if_pf (gasneti_atomic_read(&gasnetc_collective_count) != 0) \
    { gasnete_poll(); }

In the core, one would then call GASNETE_POLL_IF_NEEDED() in
gasnetc_poll(), at least in template-conduit, and perhaps at other
places where it could be helpful (such as right after an AM handler is
run).


2) Blocking thread:

To do this we just need to add a little to the case above.  Note that
we are about to add condition variables to gasnet.  They should be a
natural wrapper around pthread condition variables, taking a hsl in
place of a pthread mutex.  Note also that a handler may NEVER wait on a
condition variable, only signal it.  Prototypes for gasneti_cond*
appear in the next section.

Implement the following in gasnet_extended_fwd.h:

extern unsigned gasnetc_collective_count;
extern gasnet_hsl_t gasnetc_collective_hsl;
extern gasneti_cond_t gasnetc_collective_cond;

GASNET_INLINE_MODIFIER(gasnete_collective_entry_hook)
void gasnete_collective_entry_hook(void)
{
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasnetc_collective_count++;
  if (gasnetc_collective_count == 1) {
    gasneti_cond_signal(&gasnetc_collective_cond);
  }
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}

GASNET_INLINE_MODIFIER(gasnete_collective_leave_hook)
void gasnete_collective_leave_hook(void)
{
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasnetc_collective_count--;
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}

In the core, the thread would use something such as:

void gasnetc_collective_poll_thread(void)
{
  while (!EXITING()) {
    gasnet_hsl_lock(&gasnetc_collective_hsl);
    while (!EXITING() && !gasnetc_collective_count) {
       gasneti_cond_wait(&gasnetc_collective_cond,
                         &gasnetc_collective_hsl);
    }
    gasnet_hsl_unlock(&gasnetc_collective_hsl);

    if_pt (!EXITING()) {
      gasnete_poll();
    }
  }

  gasnetc_exit(0);
}


3) Interrupt mode

Implement the following in gasnet_extended_fwd.h:

extern unsigned gasnetc_collective_count;
extern gasnet_hsl_t gasnetc_collective_hsl;

GASNET_INLINE_MODIFIER(gasnete_collective_entry_hook)
void gasnete_collective_entry_hook(void)
{
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasnetc_collective_count++;
  if (gasnetc_collective_count == 1) {
    ENTER_INTERRUPT_MODE();
  }
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}

GASNET_INLINE_MODIFIER(gasnete_collective_leave_hook)
void gasnete_collective_leave_hook(void)
{
  gasnet_hsl_lock(&gasnetc_collective_hsl);
  gasnetc_collective_count--;
  if (gasnetc_collective_count == 0) {
    LEAVE_INTERRUPT_MODE();
  }
  gasnet_hsl_unlock(&gasnetc_collective_hsl);
}




It might be tempting to merge the common ideas in #2 and #3 in to an
"edge triggered" pair of hooks, one called on the 0->1 transition and
the other on the 1->0 transition.  However, this would not work very
cleanly with the condition variable unless the hsl used to trigger the
hooks was exposed for use in the waiting thread.  I dislike that as an
unclear interface, and the code duplicated between #2 and #3 is too
small to justify such a design.


=============================================================================
* Proposal for gasneti_cond_t:

In order to permit a conduit to block waiting for something that might
happen in another thread, particularly in a handler executed in
another thread, we want something like pthread_cond_t, but which takes
an hsl in place of a pthread_mutex_t.  Note that these are for use
INTERNALLY, and we don't want to add them to the GASNet spec where
client code might use them.  (At least *I* don't want to do so)

Here is an attempt (not run through the compiler) to implement
pthread_cond_t in terms of our "reference" hsl, which is just a struct
holding a gasneti_mutex_t:

typedef
struct _gasneti_cond_t {
  pthread_cond_t cond;
} gasneti_cond_t;

#define GASNETI_COND_INITIALIZER { PTHREAD_COND_INITIALIZER }

#define gasneti_cond_init(P) pthread_cond_init(&((P)->cond))
#define gasneti_cond_destroy(P) pthread_cond_destroy(&((P)->cond))
#define gasneti_cond_signal(P) pthread_cond_signal(&((P)->cond))

#define gasneti_cond_wait(P,H) gasneti_cond_wait_mutex(P, &((H)->lock))

extern void gasneti_cond_wait_mutex(gasneti_cond_t *cond,
                                    gasneti_mutex_t *lock);



void
gasneti_cond_wait_mutex(gasneti_cond_t *cond, gasneti_mutex_t *lock)
{
  #if !GASNETI_USE_TRUE_MUTEXES
    gasneti_fatalerror(
       "Can't wait on condition variable w/o true mutexes");
  #endif

  #if GASNET_DEBUG
    gasneti_assert(lock->owner == GASNETI_THREADIDQUERY());
    lock->owner = (uintptr_t)GASNETI_MUTEX_NOOWNER;

    pthread_cond_wait(&(cond->cond), &(lock->lock));

    gasneti_assert(lock->owner == (uintptr_t)GASNETI_MUTEX_NOOWNER);
    lock->owner = GASNETI_THREADIDQUERY();
  #else
    pthread_cond_wait(&(cond->cond), lock);
  #endif
}



=============================================================================
* Notes on algorithms 

Broadcast:
==========
  In [1] we learn how to construct an optimal schedule for broadcast.  The
tree can be built knowing only the ratio A=ELL/max(O_s,g).  Using a heap data
structure one can determine the tree structure for a P-node broadcast in
O(log(P!)) time.  Note that O(P) < O(log(P!)) < O(P*log(P)), so the cost to
construct the schedule grows superlinearly.  However, it doesn't require any
communiction (deterministic algorithm allows all nodes to compute the same
schedule independently).
  For the case of A>>1 the communication is latency dominant and the optimal
broadcast is the naive one in which the root sends data to each other node in
sequence.  For the case of A near 1, the optimal tree shape is a Binomial
tree[2].  Here are, for P=8, model results for the "EEL" of a broadcast with
four different tree shapes on three different networks.  Note that these are
in us and are directly comparable between networks.  However, they are just
model results, not real timings.  The "Flat" tree is the one-level tree of
the naive broadcast.  The percentages are relative to the optimal.

		gm		vapi		mpi-conduit
Optimal		21		24		334
Binomial	21 		30 (+28%)	570 (+171%)
Binary		26 (+24%)	30 (+28%)	570 (+171%)
Flat		43 (+105%)	31 (+32%)	334

The binary tree and the "flat" tree are the easiest two to implement.  With
just these two choices, we can get within 30% of the optimal on the two fast
networks, and the flat tree is the optimal for the slow network.  I don't
have any bound on just how bad we could do with these two, or any heuristic
for choosing between them.  However, I expect to implement them as the first
two versions.

Later I hope to implement the construction of the optimal trees.  I can see
doing it two ways:
1) At init time for some fixed A, ignoring the data size.
2) At runtime, using some size-dependent (and conduit-specific) model for A
   and caching the results so it can be reused when the same size is repeated.
   Note that the data to cache would just be P*sizeof(gasnet_node_t) bytes.


Scatter:
========

I am planning to examine the use of the optimal broadcast tree to do a
tree-based scatter.  However, the fact that timings will NOT remain fixed as
the payload is changed (sending more data near the root than at the leaves)
means that the simple method for building such trees is only approximately
applicable.  There is work proving that building optimal schedules for some
tree-structured problems is NP-hard[4].  However, I don't yet know if this
particular problem is one of them or not.


Gather:
=======

The Gather is exactly the same as scatter with all the data moving in the
opposite direction.  Therefore whatever I decide for Scatter will also be used
to implement Gather.


Exchange:
=========

The final data placement is equivalent to doing P scatters, one with each
node as the root.  In a simple LogP model where we can only send a single data
item per communication operation, the optimal[1] is to perform P naive scatters
in lock-step, but chosing the order of each node's sends so each node receives
exactly once per step.  The simplest such ordering is for node p to send to
((p+1) mod P), ((p+2) mod P),... ((p+P-1) mod P).


Gather_All:
===========

The final data placement is equivalent to doing P broadcasts.  In the LogP
model with single-item sends the optimal[1] is the same as described above
for Exchange, but with the same data item instead of distinct ones.

If we allow a node to send more than a single data item per send, then we can
improve on the methods above.  If we divide the communication into ceil(lg(P))
rounds (where lg denoted base-2 log) then each round can forward an
exponentially increasing amount of data (1 item in round 0, 2 items in round
1, 2^i items in round i).  At the end of the final round each node has
received 2^i items.  With the correct pairings for these sends, we can ensure
that all P <= 2^i items are received at each node (and with a little care we
can even avoid sending the "surplus" 2^i-P that exists when P is not a power
of two).  The ordering for this is that in round i (starting from 0) node p
will send to node ((p+2^i) mod P).  As it happens, this is exactly the same
communication pattern used in the Dissemination barrier[3; sec 3.3], but now
we are also moving real data.

For small P the Exchange-like algorithm is faster, even if we assume the
timings are constant as the payload size grows in each round.  As the ratio A
(=ELL/max(O_s,g)) increases the crossover point moves toward larger P.  The
dominant term for the Exchange-like algorithm is O(P*max(O_s,g)), due to
the gap between the P sends from each node.  The dominant term for the
Dissemination-like algorithm is O(lg(P)*ELL), due to the need to wait for
data to arrive (before resending it) at each of the lg(P) rounds.
Both algorithms move the exact same number of data bytes in and out
of each node.  The Dissemination-like algorithms tries to reduce the number of
sends needed to actually move the data, but must pay a cost proportional to
ELL to wait for the needed data to arrive.


Permute:
========

Other than implementing the UPC synchronization, there is no obvious mechanism
by which GASNet can do any better than the use of Puts.


Reduce:
=======

As noted in [1], the optimal communication pattern for reduction in the LogP
model is the same tree as for Broadcast.  The order and direction of the comms
are reversed and the data elements moving are partial "sums" from each subtree.
There is no big concern over unequal contributions here, since each node must
contribute exactly 1 or 0 "sums" to the total reduction.  The associativity
requirement on operators makes things easier for us.

The UPC_NONCOMM_FUNC case might take a little extra work, since it would
restrict the trees we could legally use.  I've not tried to figure out what
the actual restriction would look like.


PrefixReduce:
=============

The UPC collectives spec allows one to take a slice of a UPC array such that
the number of elements can differ among the nodes.  The basic method I have
in mind would have each node compute the sums over each block (trivial for
blocksize==1) and use a Gather_All to transfer these sums.  To deal with the
possiblly unequal contribution we could either pad the block used in the
Gather_All, or we could implement Gather_All for varible-sized contributions
(which is already under consideration).

I have yet to start searching for literature specific to this problem, so
there may be methods much better than the Gather_All desribed above.


Sort:
=====

I've no bright ideas here.  The most naive method would seem to be Gather,
Sort on one node, Scatter.  However, the Gather and Scatter want equal
contribution from each node, and Sort does not require it.  This might be
another case for implementing collectives with variable-sized contributions.

I have yet to start searching for literature specific to this problem, so
there may be methods much better than the Gather+Scatter desribed above.


Biblography:
[1]  Karp, Sahay, Santos and Schauser "Optimal Broadcast and Summation in the LogP Model"
[2]  Kielmann, Hofman, Bal, Plaat and Bhoedjang "MagPIe: MPI's Collective
     Communication Opaertaions for Clustered Wide Area Systems."
[3]  Mellor-Crummey and Scott "Algorithms for Scalable Synchronization on
     Shared-Memory Multiprocessors."
[4]  Verriet "Scheduling tree-structured programs in the LogP model"
