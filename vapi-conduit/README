README file for vapi-conduit
================================
Paul H. Hargrove <PHHargrove@lbl.gov>
$Revision: 1.16 $

@ TOC: @
@ Section: Bootstrapping @
@ Section: Build-time Configuration @
@ Section: Runtime Configuration @
@ Section: Core API @
@ Section: Extended API @
@ Section: Graceful exits @
@ Section: TO DO @


@ Section: Bootstrapping @
  
  There are two options for bootstrapping a GASNet application when
  using vapi-conduit.  You can choose by passing
      --with-vapi-bootstrap=ssh
  or  --with-vapi-bootstrap=mpi
  where mpi is the default.


  If ssh is chosen, then the following apply:
  + usage summary:
      your_app [-v] N[:M] [args ...]
    where N is the number of processes to run, and M is the number of
    nodes/hosts over which the processes will be distributed.  If only
    N is given, then M=N by default.
    If running a UPC or Titanium application, then language-specific
    commands should be used instead.
  + A list of hosts is specified using either the GASNET_SSH_NODEFILE
    or GASNET_SSH_SERVERS environment variables.
    If set, the variable GASNET_SSH_NODEFILE specifies a file with one
    hostname per line.  Blank lines and comment lines (using '#') are
    ignored.
    If set, the variable GASNET_SSH_SERVERS itself contains a list of
    hostnames, delimited by commas or whitespace.
    If both are set, GASNET_SSH_NODEFILE takes precedence.
    Note that if starting a job via upcrun or tirun, these variables
    may be set for you from other sources.
  + The environment variable GASNET_SSH_CMD can be set to specify a
    specific remote shell (perhaps rsh).  The default is "ssh", and
    a search of $PATH resolves the full path.
  + The environment variable GASNET_SSH_OPTIONS can be set to
    specify options that will precede the hostname in the commands
    used to spawn jobs.  One example, for OpenSsh, would be
      GASNET_SSH_OPTIONS="-o 'StrictHostKeyChecking no'"
  + For the following, the term "compute node"  means one of the hosts
    given by GASNET_SSH_NODEFILE or GASNET_SSH_SERVERS, which will run
    an application process.  The term "master node" means the node from
    which the job was spawned.  The master node may or may not be one
    of the compute nodes.
  + The ssh (or rsh) at your site must be configured to allow logins
    from the master node to compute nodes, and among the compute nodes.
    These must be achieved without interaction (such as entering a
    password or accepting new host keys).
  + Any firewall or port filtering must allow the ssh/rsh connections
    described above, plus TCP connections on untrusted port (those
    with numbers over 1024) from a compute node to the master node and
    and among compute nodes.
  + Resolution for all given hostnames must be possible from both the
    master node and the compute nodes.
  + THE SSH SPAWNER MAY NOT WORK WITH SOME OLDER VAPI VERSIONS WHICH
    DON'T ALLOW A VAPI APPLICATION TO CALL fork().


  If MPI is chosen, then the following apply:
  + usage summary:
      mpirun -np N your_app [args ...]
    where N is the number of processes to run.
    If running a UPC or Titanium application, then language-specific
    commands should be used instead.
  + In order to bootstrap vapi-conduit, a working MPI must be installed
    and configured on your system.  See mpi-conduit/README for
    information on configuring GASNet for a particular MPI.  Note that
    you must compile mpi-conduit as well (even if you never plan to use
    it).
  + Since that MPI is only used in gasnet_init(), gasnet_attach() and
    gasnet_exit() and not for any GASNet communications.  Therefore
    it is acceptable to use a TCP/IP based MPI such as MPICH or LAM/MPI.

@ Section: Build-time Configuration @

  By default vapi-conduit ensures network attentiveness (timely
  processing of incoming AMs) by spawning an extra thread that
  remains blocked until the arrival of an Active Message.  One
  can disable this thread by configuring GASNet with the flag
  '--disable-vapi-rcv-thread'.  It is recommended that one NOT
  use this option, but instead disabled the thread at runtime
  (see Runtime Configuration section).  If the extra thread will
  never be needed, disabling it at build time will yield a small
  reduction in latencies by allowing some locking operations to
  compile away.

  There are two known bugs in older InfiniHost firmware versions.
  Normally you shouldn't need to do anything special with regards
  to them.  However, if you do need to override the defaults the
  remainder of this section explains the issues.

  + Thread safety of VAPI_poll_cq()
    In firmware versions prior to 3.0, the VAPI_poll_cq() call is
    not thread safe under some conditions.  For this reason,
    vapi-conduit will check the firmware version at runtime and will
    use its own mutex around calls to VAPI_poll_cq() if needed.

    If you believe this bug is appearing in firmware 3.0 or later,
    you can force use of this mutex by configuring GASNet with the
    flag '--enable-vapi-force-poll-lock'.

  + Performance loss in EVAPI_post_inline_sr()
    In firmware versions after 1.17 and prior to 3.0, there is a
    performance anomaly with the call EVAPI_post_inline_sr().
    By default, vapi-conduit will use this call to perform small
    puts.  If firmware in the range [1.18, 3.0) is detected at
    runtime, a warning will be printed but the call will still
    be used.

    If you believe this bug is affecting your performance under
    other firmware versions, or to silence this warning without
    updating your firmware, configure GASNet with the flag
    '--disable-vapi-inline-puts'.  This will disable the use of
    the EVAPI_post_inline_sr() call, and disable the warning.
    
@ Section: Runtime Configuration @

  There are a number of parameters in vapi-conduit which can be tuned
  at runtime via environment variables.  There are two categories of
  parameters: connection settings and resource usage parameters.

  Connection settings:
  Under normal conditions, the Host Channel Adapter and Port will be
  located automatically.  However, in the event you have multiple
  adapters or multiple active ports on a single adapter, you may need
  to set these environment variables to identify the correct HCA and
  Port.

  These parameters may legally take different values on each node.

  + GASNET_HCA_ID
    This gives the "HCA ID" string of the InfiniBand Host Channel
    Adapter (HCA) to open.  This string is assigned by the device driver
    at boot time.  A typical HCA ID is "InfiniHost0", for the first HCA
    with the InfiniHost chip.
    The default value is an empty string, which causes vapi-conduit to
    try all HCAs detected by the device driver and use the first one
    which can be opened.

  + GASNET_PORT_NUM
    This gives the integer port number to use on the HCA.  The "port"
    is a physical port on the HCA, not to be confused with IP port
    numbers.  IB defines the port numbers to start at 1.
    The default value is 0, which causes vapi-conduit to probe the HCA
    for the lowest-numbered physical port which is configured as ACTIVE.

  + GASNET_RCV_THREAD
    This gives a boolean: "0" to disable or "1" to enable (1) the use
    of an extra thread that blocks waiting for vapi to wake it when an
    Active Message request arrives.  This allows vapi-conduit to be
    more attentive to incoming AMs even while the application may not
    be making any calls to GASNet.  The down side is that each time
    this thread wakes it must contend for CPU resources.  Thus for an
    application that is calling GASNet sufficiently often, use of this
    thread may significantly INCREASE running time.  However, on an SMP
    where an otherwise idle processor is available the use of this
    thread can REDUCE running time by relieving the application thread
    of the burden of servicing incoming AM requests and replies.
    Note that if '--disable-vapi-rcv-thread' was specified at build time
    then the extra thread is unavailable and this environment variable
    is ignored.
    Currently the default is enabled (1), but this is subject to change
    in a future version.

  Resource usage parameters:
  The following environment variables control how much memory is
  preallocated at startup time to serve various functions.  Because these
  resource pools do not grow dynamically, it is important that these
  parameters be sufficiently large, or performance degradations may
  results.  The default settings should be sufficient for most conditions.
  You may need to lower some values if you have insufficient memory.

  These parameters must be equal across all nodes, and the behavior
  otherwise is undefined.

  + GASNET_OP_OUST_PP
    This gives the maximum number of outstanding ops (RDMA + AMs) which
    can be in-flight simultaneously from a node to each of its peers.
    Here "in-flight" means queued to the send work queue and not yet
    reaped from the send completion queue.  Note that these operations
    are reaped in FIFO order, independent of when any corresponding
    GASNet operation is synced.
    This value is the depth of each send work queue.  Thus the memory
    consumed for send work queues scales as GASNET_OP_OUST_PP*(N-1),
    on each node.
    The default value is 64.
    Reducing this parameter may limit small message throughput.  If you
    believe your small message throughput is too small, you may try
    increasing this value.

  + GASNET_OP_OUST_LIMIT
    This gives the maximum number of outstanding ops (RDMA + AMs) which
    can be in-flight simultaneously from each node, with "in-flight"
    defined as in GASNET_OP_OUST_PP.
    This is (potentially) the depth of the send completion queue and the
    number of 'sreq' data structures allocated to track in-flight ops.
    However, for a given value of GASNET_OP_OUST_PP and N, it might not
    be possible to launch GASNET_OP_OUST_LIMIT operations.  Therefore,
    the memory consumed for the send completion queue and sreqs scales
    as min(GASNET_OP_OUST_LIMIT, GASNET_OP_OUST_PP*(N-1)), on each node.
    The default value is 1024.
    Reducing this parameter may limit small message throughput.  If you
    believe your small message throughput is too small, you may try
    increasing this value.

  + GASNET_AM_OUST_PP
    This give the maximum number of outstanding AM Requests which can
    be in-flight simultaneously from a node to each of its peers.  Here
    "in-flight" means the Request is queued to the send work queue, but
    the matching Reply has not yet been processed for AM flow control
    (described in another section of this README).
    This is the number of receive buffers which must be preposted to each
    endpoint for AM Requests.  With the current flow-control scheme the
    same number of additional receive buffers are needed to accept replies.
    Thus, the memory consumed for the receive work queues and the receive
    completion queue each scale as GASNET_AM_OUST_PP*(N-1) on each node.
    The default value is 32.  (256KB*(N-1) for receive buffers)
    Reducing this parameter may limit Active Message throughput.  If you
    believe your Active Message throughput is too small, you may try
    increasing this value.

  + GASNET_AM_OUST_LIMIT
    This give the integer number of outstanding AM Requests which can
    be in-flight simultaneously from each node, with "in-flight" defined
    as in GASNET_AM_OUST_PP.
    With the current flow-control scheme this parameter is not yet fully
    implemented.  Intended to control the size of the receive completion
    queue, this parameter will be used with the "dynamic flow-control"
    scheme (elsewhere in this README).  For now it is an error to set
    this parameter to anything smaller than GASNET_AM_OUST_PP*(N-1), and
    values larger than that are clipped to this value.
    The default is 32767, and represents the limit of 65535 completion
    queue entries supported by the InfiniHost firmware.
    Reducing this parameter may limit Active Message throughput.  If you
    believe your Active Message throughput is too small, you may try
    increasing this value.

  + GASNET_AM_SPARES
    This gives the number of "spare" receive buffers which are allocated
    in addition to those needed to satisfy the settings of the parameters
    GASNET_AM_OUST_PP and GASNET_AM_OUST_LIMIT.  These spare receive
    buffers are used while running AM handlers, to aid the flow-control
    process.
    Because a "spare" receive buffer is used while an AM handler is
    running, this parameter limits the number of AM handlers which can
    run concurrently before resorting to copying of an incoming AM to
    a temporary space.
    Since a given thread can only run one handler at a time, values of
    GASNET_AM_SPARES larger than the number of threads running concurrently
    in GASNet offer no extra benefit.  So, the default value depends on
    the threading configuration.  When compiled for GASNET_PARSYNC or
    GASNET_SEQ, the default is 2 to allow for a single client thread in
    GASNet plus the internal AM receive thread.  When compiled for
    GASNET_PAR, the default is 4.  Reducing this parameter may limit the
    performance of concurrent servicing of Active Messages in a GASNET_PAR
    configuration.  If you have many application threads you may wish to
    increase this value to as large as (num_threads + 1).

  + GASNET_BBUF_LIMIT
    This gives the maximum number of pre-pinned bounce buffers allocated
    on each node.  These buffers are needed for assembly of AM headers
    (and the payload of mediums), and for gets and puts with the local
    address outside the GASNet segment.  The default is 1024, which results
    in a limit of 4MB (unless GASNETC_BUFSZ is changed at compile time).
    The actual number of buffers allocated maybe less if the values of
    GASNET_OP_OUST_PP and GASNET_AP_OUST_LIMIT are such that all
    GASNET_BBUF_LIMIT buffers could never be consumed.
    Reducing this parameter limits the outstanding number of operations
    which consume bounce buffers.  This includes all Active Messages, gets
    with a destination outside the GASNet segment, and puts with a source
    outside the GASNet segment (excluding those of size less than or equal
    to GASNETC_PUT_INLINE_LIMIT which is currently 72 bytes).  If you
    believe that throughput of these operations is too small, you may try
    increasing this value.
   
@ Section: Core API @

+ Flow-control for AMs.

  The AMs in vapi-conduit are just implemented as send/recv traffic.
  Therefore a send without a corresponding recv buffer preposted at the
  peer will be stalled by the RNR (receiver-not-ready) flow control
  in IB.  However there are two reasons why we want to avoid this
  situation.  The first is that if such a send is blocked by flow
  control, then the ordering semantics of IB tell us that all the
  gets and puts that we've initiated after the AM was sent are also
  stalled.  Rather than let that happen, we should manually delay
  those which are dependent on the AM.  The second reason is that
  under some conditions the RNR flow control is very poor.  The problem
  is that once the intended receiver sends a RNR NAK to indicate no
  available recv buffers, IB has the SENDER's hardware/firmware poll
  for the receiver to become ready again!  That leaves us with a choice
  between configuring a small polling interval and consuming a lot of
  bandwidth for this polling, or a large interval which leads to 
  performance which is degraded more than necessary when IB flow control
  is asserted.

  For these reasons we implement some flow control at the AM level.
  The basic idea is that every REQUEST consumes one credit on the
  sending endpoint and every REPLY grants one credit on the receiving
  endpoint.  Thus if M is the initial number of credits on each endpoint
  and every REQUEST has exactly one matching REPLY, then M becomes a
  limit on the number of un-acknowledged REQUESTS in flight on an
  endpoint.  If we want to avoid RNR conditions, then we should start
  with M credits and 2*M preposted recv buffers on each endpoint.  The
  factor of 2 is because we can consume M of the buffers on the peer's
  endpoint with our REQUESTs and an additional M for REPLYs to her
  REQUESTS.

  It is a simple matter to count the credits when a REPLY is received
  and to poll for credits when needed to send a REQUEST.  It is also
  simple to ensure the exactly-one-reply.  We already ensure that
  at-most-one reply is sent by the request handler.  Additionally we
  must check upon handler return for the case that the request hander
  sent no reply, and send one implicitly.  We just use a special
  "system category" handler, gasnetc_SYS_ack, which doesn't even run
  a handler.

  To avoid a window of time between when we send a reply (credit) and
  when we post the recv buffer, we must post the replacement recv
  buffer BEFORE running an AM REQUEST handler.  To do this we keep a
  pool of unposted recv buffers.  So, when we recv an AM, we grab a
  free recv buffer from the pool and post it to the endpoint, and run
  the handler.  We send an implicit reply if the AM was a REQUEST and
  the handler didn't send any REPLY.  Finally we take the recv buffer
  containing the just-processed AM and we return it to the unposted
  pool.

  There is a corner case we must deal with when there are no spares
  left in the unposted pool.  In this case we will copy the received
  message into a temporary (non-pinned) buffer before processing it.
  This allows us to repost the recv buffer immediately.  Since the
  temporary buffer is not pinned, it cannot be used for receives.
  Therefore, we free the temporary buffer when the handler is done,
  rather than placing it in the unposted pool.
  
  If we reap multiple AMs in a single Poll, then we reuse the
  previous buffer as the "spare" for the next one, in place of
  grabbing one from the unposted pool each time.  Thus, we touch the
  unposted pool at most twice per Poll, once for the first AM we
  receive and once at the end to put the recv buffer of the final AM
  back in the unposted pool.  For the dedicated receive thread we can
  do even better, never touching the unposted pool at all, by always
  keeping a single thread-local "spare", initially acquired at startup.

@ Section: Extended API @

Notes for myself for extended API:

+ The send completion facility consists of two pointers to counters,
  associated with each sbuf.  If these pointers are non-NULL then the
  counter is decremented atomically when the send is complete.
  
  One counter is for awaiting reuse of local memory and is
  only be used for sbufs which are doing zero copy.  This counter
  provides the mechanism for Longs and non-bulk puts to block before
  they return, and should be allocated as an automatic variable.

  The second counter is for request completion and should be non-NULL
  for every sbuf for which request completion would be checked (all
  gets & puts, but not the Longs).  For nb and nbi the counter is
  waited on at sync-time.  Therefore the explicit handle is a struct
  containing the counter.
  
+ Similar to the reference implementation's cut-off between Mediums
  (which typically do a source-side copy) and Longs (which may not),
  we have a cut-off size, below which the RDMA-put operation will do
  source-side copies _iff_ local completion is desired (Long, put_nb,
  and put_nbi).

+ The gets are done w/ RDMA-reads, and use the sbuf bounce buffers
  if the local memory is not in the segment (or otherwise registered).
  The value gets also pass though the bounce buffers.  Clearly there
  is no bulk/non-bulk distinction in terms of local memory reuse, just
  the alignment and optimal size distinctions.  So, only the outstanding
  request counter on the sbuf is needed for syncs of all types of gets.

+ Table of when synchronization is needed
	              Local Remote
	Operation     Sync  Sync
	--------------------------
	LongAsync       X     X
	Long            I     X

	put_nb          I     S
	put_nbi         I     S
	put_nb_bulk     X     S
	put_nbi_bulk    X     S
	put_nb_val	X     S
	put_nbi_val	X     S
	put             X     I
	put_bulk        X     I
	put_val         X     I

	get_nb		X     S
	get_nbi		X     S
	get_nb_bulk	X     S
	get_nbi_bulk	X     S
	get_nb_val	X     S
	get_nbi_val (DOES NOT EXIST)
	get		X     I
	get_bulk	X     I
	get_val		X     I

   X = Not needed at all (or not even applicable with _val forms)
   I = Needed before (I)nitiating function returns
   S = Needed before (S)ynchronizing function returns

+ Some minor tweaks are used to avoid allocation of counters in
  some cases.
  - For all the functions which require waiting on a counter in the
    initiating function, the counter can be allocated on the stack (as
    an automatic variable).
  - For the implicit-handle forms the request counter is in the
    thread-specific data, possibly in an access-region.
  - For the explicit handle forms the request counter must be allocated
    from some pool, requiring some memory management work.  This is
    done with a modification to the code from the the reference
    implementation, and uses thread-local data to avoid locks.

+ The memsets can be more efficiently implemented as a _local_ memset
  followed by a RDMA put, for small enough sizes.  The cutoff is
  presently the size of one bounce buffer, but has not been tuned.
  Larger memsets are still done by Active Messages.
  

@ Section: Graceful exits @

On June 24, 2003 vapi-conduit now passes all 9 (I added two recently)
of the cases in testexit.  By "Pass" I mean that the entire gasnet job
(tested up to 8-way across my 4 dual-processor machines) terminates
with no orphans, and with tracing properly finalized (if tracing is
enabled).  On August 11, 2003 the graceful exit code was revised to
send O(N) network traffic in the worst case, as opposed to the O(N^2)
required in all cases in the first implementation.

Additionally, the exit code is properly propagated through the
bootstrap, to yield a correct exit code for the parallel job as a
whole.  If using MPI for bootstrapping, the actual exit code will
depend on supported in a given MPI implementation (some ignore the
exit code of the individual processes).

This code is heavily commented, but for the curious, here is a
description of the code.

There are three paths by which an exit request can begin.  The first
is through gasnetc_exit(), which may be called by the user, by the
conduit in certain error cases, and by the default signal handler for
"termination signals".  The second is via a remote exit request,
passed between nodes to ensure full-job termination from
non-collective exits.  The third is via an atexit() handler,
registered by gasnetc_init(), used to catch returns from main() and
user calls to exit().

There are slight variations among the code in these three cases, but
most of the work is common, and is performed by three functions:
gasnetc_exit_head(), gasnetc_exit_body() and gasnetc_exit_tail().  The
first of these, _head, is used to determine the "first" exit and store
its exit code for later use.  This is important because even a
collective exit will involve receiving remote exit requests.  Only if
a remote exit request is received before any local calls to
gasnetc_exit(), should the request handler initiate the exit.  Note
that even in the case of a collective exit it is possible for the
first remote request to arrive before the local gasnetc_exit() call.
However, that is made very unlikely by the timing and is nearly
harmless since the only difference is the raising of SIGQUIT in
response to a remote exit request, which is not done for
locally-initiated ones.

The second common function, _body(), is used to perform the "meat" of
the shutdown.  It begins by ignoring SIGQUIT to avoid re-entrance, and
then blocks all but the first caller in a polling loop to avoid
multiple threads from executing the shutdown code.  Because strange
things can happen if we are trying to shutdown from a signal context,
a signal handler is installed for all the "abort signals".  This
signal handler just calls _exit() with the exit code stored by
_head().  Because we may have problems shutting down if certain locks
were held when a signal arrived, we also install the signal handler
for SIGALRM, and use the alarm() function to bound the time spent
blocked in the shutdown code.  While there is the risk that this alarm
might go off "too soon" if the shutdown has lots of work to do, we can
be certain that the correct exit code is still generated.

Once the signal handlers are established, _body closes down the
tracing and stats gathering and flushes stdout and stderr.  Then _body
calls gasnetc_get_exit_role() to "elect" a master node for the exit.
This is done with an alarm() timer in force.  The use of an "election"
with a timeout ensures that we will exit, even if node 0 is wedged.
The election of a master proceeds by sending a system-category AM
request to node 0, and spinning to wait for a corresponding reply,
which will indicated if the local node is the "master" or a "slave"
in the coordination of the graceful exit.  The logic on node 0
ensures that the first "candidate" is always made the master, not
waiting for multiple AMs to arrive.  Additionally the slave nodes
may, under circumstances described below, know before entering
gasnetc_get_exit_role() that they are slaves, and will not bother
to send an AMRequest to node 0.  In either case gasnetc_get_exit_role()
indicates to _body which role the local node is to assume.

From _body, the single master node will enter gasnetc_exit_master() and
will begin sending an remote exit request (system-category AM, so this
will all work between _init and _attach) to each peer.  Then the master
waits (with timeout, of course) for a reply from each peer.  This request
conveys the desired exit code to each node.  It also will wake them out
of a spin-loop, barrier, or other case where they were not yet aware of
the need to exit.  In the handler for the exit request, a node will send
a reply back to the master, so it knows all the nodes are reachable.  It
will set its role to "slave" and, if no exit is in-progress, it will start
the exit procedure, as described later.  From _body, the slave nodes all
call gasnetc_exit_slave(), which simply spins until the remote exit request
has arrived from the master.

Regardless of whether the sending of exit requests and replies completed
within the timeout, _body proceeds to shutdown the transport and release
the conduit's resources.  This is, again, protected by an alarm() in case
we get wedged.  Once the transport resources are released, _body flushes
stdout and stderr one last time and closes stdin, stdout and stderr.
Finally, _body shuts down its bootstrap support.  If the coordination
was completed within the timeout, then the gasnetc_bootstrapFini()
routine is called indicating that we'll not be making any more calls
to the bootstrap code and expect to exit shortly.  However, if the
coordination did timeout we call gasnetc_bootstrapAbort(exitcode).  This
call is meant to request that the bootstrap terminate our job "with
prejudice" since we failed to coordinate a graceful shutdown on our
own.  We do this to try to avoid orphans, but risk lots of unsightly
error messages and possible loss of our exit code. Assuming we did not
call _bootstrapAbort (which does not return) we finish _body by
canceling our alarm timer and return to our caller.

The final common routine is gasnetc_exit_tail().  This function just
does the last bit of work to terminate the job.  It is not included in
_body because we let the atexit() case terminate "normally" after
_body returns.  However, in the case of exits initiated via
gasnet_exit() or remote exit request we call _tail to complete the
exit.  In _tail we set an atomic variable to wake any threads which
were stuck polling in _body due to being other than the first thread
to enter.  Those threads should eventually wake and also call _tail to
terminate.  Next, we call gasneti_killmyprocess() to do any platform-
specific magic required to get the entire multithreaded application to
exit.  Finally we call _exit() with the saved exit code.

Given the routines gasnetc_exit_{head,body,tail}() the code for the
three types of exit are pretty trivial.  In particular, gasnetc_exit()
just calls _head, _body and _tail with no additional logic.  In the
request handler for the exit request AM, we look at the return from
_head to determine if this exit request is the first we've seen
(inclusive of local calls to gasnet_exit() and our atexit handler).  If
it IS the first exit request, then we raise a SIGQUIT, as required by
the GASNet spec, to allow the user's handler to perform its cleanup.
However, to get the most robust exit code we don't want to run the
_body code from a signal handler context if we can avoided it.
Therefore we inspect the signal handler and skip the raise() call if
the handler is the gasnet default handler, SIG_DFL or SIG_IGN.  After
the raise() returns, or is skipped all together, we are certain that
the user's hander, if any, has executed and has NOT called
gasnet_exit().  If a user handler had called gasnet_exit(), then
raise() would not have returned.  So, if we reach the code after the
possible raise(), we proceed to call gasnetc_exit_body() and _tail to
complete the (hopefully) graceful exit of the gasnet job.

It is important to note that if we get a remote exit request that
initiates an exit, then we will never return from the handler.
However, the design of the AM code in VAPI conduit ensures that this
will actually work without deadlock.  For one, we never run handlers
from signal context or with locks held.  Thus we can expect a
"clean" set of locks.  Furthermore, we don't expect to do anything
useful with the network once the request handler calls _body anyway.

The atexit handler just calls _head and _body before returning to
allow the exit to complete.  In this case we have a little problem
with the lack of access to the return code.  Therefore we just pass 0
to _head, which _body then sends in the remote exit requests.
Experience has shown that, at least with LAM/MPI for bootstrap, when
all but one task exits with zero, the single non-zero exit code
becomes the exit code for the parallel job.  Therefore, using zero
here gives the specified exit code from the parallel job for both
collective and non-collective returns from main.

In the best case one node is way ahead of the others and can win the
master election and send remote exit requests before the others attempt
the election.  In this case the coordinated shutdown needs 1 round-trip
for the election, followed by (N-1) round-trips for the remote exit
request/reply, for a total of 2*N AMs sent.

In the worst case all nodes attempt the election at roughly the same time
and a full N round-trips take place for the election, followed by (N-1)
round trips for the remote exit request/reply, for a total of 4*N-2 AMs
sent.

The average case is somewhere between these two.

@ Section: TO DO @

+ Value gets move as follows:
  - From network to bounce buffer
  - From bounce buffer to _gasnet_valget_op_t or automatic variable
  - From memory to return from function
  The bounce buffer is a little harder to eliminate than for the puts,
  unless we can allocate the _gasnet_valget_op_t in pinned memory.  One
  way to do this might be for the _gasnet_valget_op_t to be placed IN
  the bounce buffer, but that creates a problem with long-lived bounce
  buffers with non-blocking gets.  This _will_ work, however, for blocking
  value gets.

+ Can use "notifn" to make the progress thread only wake if there are
  multiple outstanding recvs.  This could, for instance, wake only if
  we reach the RNR state (or maybe are one event shy of it).
 
+ There is DDR memory on the board, which is at least 2.5 times faster
  than host memory from the point-of-view of the network.  The down side
  is that it is both slower and non-cachable from the host.  However, it
  might be sensible to use it for some purposes.  In particular bounce
  buffers don't need to be cachable.  So, the bounce buffers could move
  to the HCA.

  I speculate that the result could be an increase in overhead, due to
  the host-copy going to slower memory.  However, I expect that the total
  end-to-end latency may be reduced slightly and less cache pollution will
  occur.  Additionally the "background" copy by the HCA will not cause
  traffic on either the PCI or memory bus - reducing resource contention
  during the time one may wish to overlap computation.

    PUT Case I: 
      CPU copies host_mem->host_mem
      HCA copies host_mem->HCA_mem
      HCA sends from HCA_mem to network
    PUT Case II:
      CPU copies host_mem->HCA_mem
      HCA copies HCA_mem->HCA_mem (Might be omitted?)
      HCA sends from HCA_mem to network
 
    GET Case I:
      HCA recvs to HCA_mem
      HCA copies HCA_mem->host_mem
      CPU copies host_mem->host_mem
    GET Case II:
      HCA recvs to HCA_mem
      HCA copies HCA_mem->HCA_mem (Might be omitted?)
      CPU copies HCA_mem->host_mem

  Of course I can't get a program to allocate from HCA memory!

+ Adaptive flow-control for AMs.
  In this mode we'd prepost just M recv buffers to each QP if we want to
  have up to M AM's outstanding per peer, rather than 2*M as we do now.
  The present use of 2*M ensures one available for each matching reply
  (though coming in the opposite direction).  Since the buffers for the
  replies are needed LOCALLY, we can be adaptive and post one extra
  buffer before each AM request rather than preposting them.
  We need to maintain a shared pool of these unposted buffers and be
  prepared to poll for them to become available along with polling
  for available credits.  This pool would probably be combined with
  the pool of "spares".
